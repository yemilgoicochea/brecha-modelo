{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c142f21",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è ORDEN DE EJECUCI√ìN DE CELDAS\n",
    "\n",
    "**Si acabas de reiniciar el kernel, sigue este orden:**\n",
    "\n",
    "1. **CELDA 1**: Importar librer√≠as y configurar directorio\n",
    "2. **CELDA 2**: Cargar datos (brechas.csv, proyectos.csv)\n",
    "3. **CELDA 3**: Generar embeddings de brechas\n",
    "4. **CELDA 4**: Crear √≠ndice FAISS\n",
    "5. **CELDA 5**: Preparar etiquetas multi-label\n",
    "6. **CELDA 6**: Crear dataset de PyTorch\n",
    "7. **CELDA 7**: ‚ö° **ENTRENAR MODELO** (10-60 min seg√∫n GPU)\n",
    "8. **CELDA 8**: Clasificaci√≥n h√≠brida (requiere modelo entrenado)\n",
    "9. **CELDA 9**: Integraci√≥n LLM (opcional)\n",
    "\n",
    "**IMPORTANTE:** No puedes ejecutar CELDA 8 sin antes completar CELDA 7.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac26d1",
   "metadata": {},
   "source": [
    "## üìö Recursos y Documentaci√≥n de Modelos\n",
    "\n",
    "### Modelos utilizados en este notebook:\n",
    "\n",
    "#### üîπ **BETO (BERT Espa√±ol) - Clasificaci√≥n**\n",
    "- **Repositorio:** [dccuchile/bert-base-spanish-wwm-cased](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased)\n",
    "- **Tipo:** Modelo BERT pre-entrenado en espa√±ol con Whole Word Masking\n",
    "- **Uso:** Clasificaci√≥n multi-label de proyectos en brechas\n",
    "\n",
    "#### üîπ **MPNet - Embeddings Sem√°nticos**\n",
    "- **Repositorio:** [paraphrase-multilingual-mpnet-base-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2)\n",
    "- **Tipo:** Modelo de embeddings multiling√ºe (768 dimensiones)\n",
    "- **Uso:** B√∫squeda por similitud sem√°ntica de brechas\n",
    "\n",
    "#### üîπ **Alternativas de Modelos de Embeddings:**\n",
    "- [MiniLM](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) (384 dims, m√°s r√°pido)\n",
    "- [Modelo Espa√±ol espec√≠fico](https://huggingface.co/hiiamsid/sentence_similarity_spanish_es) (768 dims)\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ Documentaci√≥n General:\n",
    "\n",
    "- **HuggingFace Transformers:** https://huggingface.co/docs/transformers\n",
    "- **Sentence-Transformers:** https://www.sbert.net/\n",
    "- **FAISS (b√∫squeda vectorial):** https://github.com/facebookresearch/faiss/wiki\n",
    "- **PyTorch:** https://pytorch.org/docs/\n",
    "\n",
    "### üîç Buscar m√°s modelos:\n",
    "\n",
    "- **Hub de modelos:** https://huggingface.co/models\n",
    "  - Filtrar por idioma: `Spanish` o `Multilingual`\n",
    "  - Filtrar por tarea: `Text Classification`, `Sentence Similarity`\n",
    "- **Leaderboard MTEB** (comparar embeddings): https://huggingface.co/spaces/mteb/leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be9bfb3",
   "metadata": {},
   "source": [
    "# Pipeline H√≠brido de Clasificaci√≥n de Brechas\n",
    "\n",
    "## Requisitos para Desarrollo Local\n",
    "\n",
    "### Software Base:\n",
    "- **Python 3.9+** (recomendado 3.10 o 3.11)\n",
    "- **Git** para control de versiones\n",
    "- **VS Code** con extensi√≥n de Jupyter\n",
    "\n",
    "### Archivos de Datos Requeridos:\n",
    "```\n",
    "project/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ brechas.csv          # Columnas: id, brecha\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ proyectos.csv        # Columnas: project_id, title, description, brecha_ids\n",
    "‚îú‚îÄ‚îÄ models/                  # Se crear√° durante el entrenamiento\n",
    "‚îú‚îÄ‚îÄ outputs/                 # Se crear√° autom√°ticamente\n",
    "‚îî‚îÄ‚îÄ notebooks/\n",
    "    ‚îî‚îÄ‚îÄ hybrid_pipeline.ipynb\n",
    "```\n",
    "\n",
    "### Hardware Recomendado (Local):\n",
    "- **RAM**: M√≠nimo 8GB, recomendado 16GB\n",
    "- **GPU** (opcional): NVIDIA con 6GB+ VRAM para acelerar entrenamiento\n",
    "- **Disco**: 5GB+ libres para modelos y datasets\n",
    "\n",
    "---\n",
    "\n",
    "## Despliegue en Google Cloud\n",
    "\n",
    "Este notebook ser√° ejecutado en **Google Cloud Platform** con:\n",
    "- **Vertex AI Workbench** o **AI Platform Notebooks**\n",
    "- **Google Cloud Storage** para datos y modelos\n",
    "- **GPU**: Tesla T4 o V100 (configuraci√≥n en cloud)\n",
    "- **Docker containers** para producci√≥n\n",
    "\n",
    "Por ahora, instala los paquetes b√°sicos localmente para desarrollo y testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e2815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (1.12.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (70.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (1.12.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (70.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# OPCI√ìN 2: Para VS CODE (Jupyter local)\n",
    "# ========================================\n",
    "\n",
    "# IMPORTANTE: Tu sistema tiene CUDA 12.6, instalar PyTorch compatible\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Luego instalar el resto de paquetes (incluyendo accelerate para Trainer)\n",
    "%pip install -U pip\n",
    "%pip install sentence-transformers faiss-cpu transformers accelerate datasets scikit-learn pandas numpy tqdm\n",
    "\n",
    "# ‚ö†Ô∏è DESPU√âS DE EJECUTAR: REINICIA EL KERNEL del notebook (bot√≥n \"Restart\" arriba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c39e2a",
   "metadata": {},
   "source": [
    "## üì¶ Instalaci√≥n de Dependencias\n",
    "\n",
    "**Nota:** La sintaxis cambia seg√∫n el entorno:\n",
    "- **VS Code/Jupyter**: usa `%pip install`\n",
    "- **Google Colab**: usa `!pip install`\n",
    "\n",
    "Ejecuta solo UNA de las siguientes celdas seg√∫n tu entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d52047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# OPCI√ìN 1: Para GOOGLE COLAB\n",
    "# ========================================\n",
    "# Descomenta (quita el #) si est√°s en Google Colab:\n",
    "\n",
    "# !pip install -U pip\n",
    "# !pip install sentence-transformers faiss-cpu transformers accelerate datasets scikit-learn pandas numpy tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ab6e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cambiado directorio de trabajo a ra√≠z del proyecto: d:\\UPC\\SEPTIMO CICLO\\MODULO REGULAR\\Proyecto de Investigaci√≥n 1\\TRABAJO\\TP1\\Modelo\\project\n"
     ]
    }
   ],
   "source": [
    "# CELDA 1\n",
    "# ====================================================================\n",
    "# IMPORTACI√ìN DE LIBRER√çAS Y CONFIGURACI√ìN DEL DIRECTORIO DE TRABAJO\n",
    "# ====================================================================\n",
    "\n",
    "# os: M√≥dulo del sistema operativo para operaciones de archivos y directorios\n",
    "import os\n",
    "\n",
    "# pandas: Librer√≠a para manipulaci√≥n y an√°lisis de datos en estructuras tabulares (DataFrames)\n",
    "import pandas as pd\n",
    "\n",
    "# numpy: Librer√≠a fundamental para computaci√≥n cient√≠fica, manejo de arrays y operaciones matem√°ticas\n",
    "import numpy as np\n",
    "\n",
    "# tqdm: Librer√≠a para mostrar barras de progreso en bucles e iteraciones\n",
    "from tqdm import tqdm\n",
    "\n",
    "# faiss: Librer√≠a de Facebook AI para b√∫squeda de similitud vectorial eficiente (k-NN aproximado)\n",
    "import faiss\n",
    "\n",
    "# torch: Framework de deep learning PyTorch para redes neuronales y c√≥mputo tensorial\n",
    "import torch\n",
    "\n",
    "# MultiLabelBinarizer: Transforma etiquetas multi-clase en formato binario (one-hot encoding)\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# SentenceTransformer: Modelo para generar embeddings sem√°nticos de texto (vectores densos)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Transformers de HuggingFace:\n",
    "# - AutoTokenizer: Tokenizador autom√°tico que carga el apropiado seg√∫n el modelo\n",
    "# - AutoModelForSequenceClassification: Modelo pre-entrenado para clasificaci√≥n de secuencias\n",
    "# - Trainer: API de alto nivel para entrenar modelos de transformers\n",
    "# - TrainingArguments: Configuraci√≥n de hiperpar√°metros para el entrenamiento\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Path: Clase para manejo de rutas de archivos de forma multiplataforma\n",
    "from pathlib import Path\n",
    "\n",
    "# ====================================================================\n",
    "# DETECCI√ìN Y CONFIGURACI√ìN DEL DIRECTORIO RA√çZ DEL PROYECTO\n",
    "# ====================================================================\n",
    "\n",
    "# Obtener el directorio de trabajo actual donde se ejecuta el notebook\n",
    "CWD = Path.cwd()\n",
    "\n",
    "# Verificar si existe la carpeta 'data' en el directorio actual\n",
    "if not (CWD / 'data').exists():\n",
    "    # Si no existe, buscar en el directorio padre\n",
    "    parent = CWD.parent\n",
    "    \n",
    "    # Verificar si el directorio padre contiene 'data'\n",
    "    if (parent / 'data').exists():\n",
    "        # Cambiar el directorio de trabajo a la ra√≠z del proyecto\n",
    "        os.chdir(parent)\n",
    "        print(f\"[INFO] Cambiado directorio de trabajo a ra√≠z del proyecto: {parent}\")\n",
    "    else:\n",
    "        # Advertir si no se encuentra 'data' en ninguna ubicaci√≥n esperada\n",
    "        print(f\"[ADVERTENCIA] No se encontr√≥ carpeta 'data' en {CWD} ni en {parent}. Las lecturas fallar√°n si las rutas no existen.\")\n",
    "else:\n",
    "    # El directorio actual ya es la ra√≠z del proyecto\n",
    "    print(f\"[INFO] Directorio de trabajo ya est√° en ra√≠z del proyecto: {CWD}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d06e94",
   "metadata": {},
   "source": [
    "## üîç Verificaci√≥n de GPU/VRAM\n",
    "\n",
    "Ejecuta la siguiente celda para verificar si tus modelos est√°n usando la GPU (VRAM) o solo la RAM del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c767814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERIFICACI√ìN DE HARDWARE\n",
      "============================================================\n",
      "\n",
      "‚úì CUDA disponible: True\n",
      "‚úì GPU detectada: NVIDIA GeForce RTX 3060\n",
      "‚úì Versi√≥n CUDA: 12.1\n",
      "\n",
      "üìä MEMORIA GPU (VRAM):\n",
      "  - Total VRAM: 12.00 GB\n",
      "  - VRAM en uso: 0.00 GB\n",
      "  - VRAM reservada: 0.00 GB\n",
      "  - VRAM disponible: 12.00 GB\n",
      "\n",
      "üß™ Prueba de GPU:\n",
      "  - Tensor creado en: cuda:0\n",
      "  - ‚úì GPU est√° funcionando correctamente\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICACI√ìN DE HARDWARE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Verificar disponibilidad de CUDA\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\n‚úì CUDA disponible: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    # 2. Informaci√≥n de la GPU\n",
    "    print(f\"‚úì GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì Versi√≥n CUDA: {torch.version.cuda}\")\n",
    "    \n",
    "    # 3. Memoria VRAM\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    vram_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    vram_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    \n",
    "    print(f\"\\nüìä MEMORIA GPU (VRAM):\")\n",
    "    print(f\"  - Total VRAM: {total_vram:.2f} GB\")\n",
    "    print(f\"  - VRAM en uso: {vram_allocated:.2f} GB\")\n",
    "    print(f\"  - VRAM reservada: {vram_reserved:.2f} GB\")\n",
    "    print(f\"  - VRAM disponible: {total_vram - vram_reserved:.2f} GB\")\n",
    "    \n",
    "    # 4. Prueba de tensor en GPU\n",
    "    print(f\"\\nüß™ Prueba de GPU:\")\n",
    "    test_tensor = torch.randn(1000, 1000).cuda()\n",
    "    print(f\"  - Tensor creado en: {test_tensor.device}\")\n",
    "    print(f\"  - ‚úì GPU est√° funcionando correctamente\")\n",
    "    \n",
    "    # Limpiar memoria\n",
    "    del test_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è GPU NO DISPONIBLE\")\n",
    "    print(\"Los modelos se ejecutar√°n en CPU (usando RAM del sistema)\")\n",
    "    print(\"\\nPara usar GPU:\")\n",
    "    print(\"1. Instala PyTorch con CUDA:\")\n",
    "    print(\"   %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    print(\"2. Verifica que tienes drivers NVIDIA actualizados\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870e2c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch versi√≥n: 2.5.1+cu121\n",
      "CUDA build version: 12.1\n",
      "cuDNN version: 90100\n",
      "CUDA disponible: True\n"
     ]
    }
   ],
   "source": [
    "# Verificar versi√≥n exacta de PyTorch instalada\n",
    "import torch\n",
    "print(f\"PyTorch versi√≥n: {torch.__version__}\")\n",
    "print(f\"CUDA build version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4391d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brechas: 48\n",
      "Proyectos etiquetados: 817\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>INDICADOR DE BRECHA POR DEFINIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>PORCENTAJE DE ALIMENTOS AGROPECUARIOS DE PRODU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>PORCENTAJE DE CAPITALES DE DISTRITO QUE NO CUE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>PORCENTAJE DE CENTROS CUNA M√ÅS EN CONDICIONES ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             brecha\n",
       "0   1                    INDICADOR DE BRECHA POR DEFINIR\n",
       "1   2  PORCENTAJE DE ALIMENTOS AGROPECUARIOS DE PRODU...\n",
       "2   3  PORCENTAJE DE CAPITALES DE DISTRITO QUE NO CUE...\n",
       "3   4  PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTAL...\n",
       "4   5  PORCENTAJE DE CENTROS CUNA M√ÅS EN CONDICIONES ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>brecha_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CONSTRUCCION DE VEREDAS Y RECONSTRUCCION DE PI...</td>\n",
       "      <td>CONSTRUCCION DE VEREDAS Y RECONSTRUCCION DE PI...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MEJORAMIENTO DE VEREDAS EN LA AVENIDA PUYA RAY...</td>\n",
       "      <td>MEJORAMIENTO DE VEREDAS EN LA AVENIDA PUYA RAY...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MEJORAMIENTO DE LA INFRAESTRUCTURA VIAL Y PEAT...</td>\n",
       "      <td>MEJORAMIENTO DE LA INFRAESTRUCTURA VIAL Y PEAT...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MEJORAMIENTO DE AV. JORGE BASADRE GROHMANN DES...</td>\n",
       "      <td>MEJORAMIENTO DE AV. JORGE BASADRE GROHMANN DES...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>FORTALECIMIENTO DEL SERVICIO DE LIMPIEZA PUBLI...</td>\n",
       "      <td>FORTALECIMIENTO DEL SERVICIO DE LIMPIEZA PUBLI...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   project_id                                              title  \\\n",
       "0           1  CONSTRUCCION DE VEREDAS Y RECONSTRUCCION DE PI...   \n",
       "1           2  MEJORAMIENTO DE VEREDAS EN LA AVENIDA PUYA RAY...   \n",
       "2           3  MEJORAMIENTO DE LA INFRAESTRUCTURA VIAL Y PEAT...   \n",
       "3           4  MEJORAMIENTO DE AV. JORGE BASADRE GROHMANN DES...   \n",
       "4           5  FORTALECIMIENTO DEL SERVICIO DE LIMPIEZA PUBLI...   \n",
       "\n",
       "                                         description  brecha_ids  \n",
       "0  CONSTRUCCION DE VEREDAS Y RECONSTRUCCION DE PI...          18  \n",
       "1  MEJORAMIENTO DE VEREDAS EN LA AVENIDA PUYA RAY...          18  \n",
       "2  MEJORAMIENTO DE LA INFRAESTRUCTURA VIAL Y PEAT...          18  \n",
       "3  MEJORAMIENTO DE AV. JORGE BASADRE GROHMANN DES...          18  \n",
       "4  FORTALECIMIENTO DEL SERVICIO DE LIMPIEZA PUBLI...          36  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELDA 2\n",
    "# data/brechas.csv => columns: id,brecha\n",
    "# data/proyectos.csv => columns: project_id,title,description,brecha_ids (e.g. \"1,3\")\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd()  # despu√©s del ajuste en CELDA 1 debe ser la ra√≠z\n",
    "brechas_path = PROJECT_ROOT / 'data' / 'brechas.csv'\n",
    "proyectos_path = PROJECT_ROOT / 'data' / 'proyectos.csv'\n",
    "\n",
    "if not brechas_path.exists():\n",
    "    raise FileNotFoundError(f\"No se encontr√≥ {brechas_path}. Verifica que el archivo exista y que el notebook se ejecute desde la carpeta 'project' o que la celda 1 haya cambiado el cwd correctamente.\")\n",
    "if not proyectos_path.exists():\n",
    "    raise FileNotFoundError(f\"No se encontr√≥ {proyectos_path}. Verifica que el archivo exista.\")\n",
    "\n",
    "brechas = pd.read_csv(brechas_path)\n",
    "proyectos = pd.read_csv(proyectos_path)\n",
    "\n",
    "# Quick preview\n",
    "print(\"Brechas:\", len(brechas))\n",
    "print(\"Proyectos etiquetados:\", len(proyectos))\n",
    "display(brechas.head())\n",
    "display(proyectos.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34cebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (48, 768)\n",
      "Modelo usado: N/A\n",
      "Dimensiones por embedding: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELDA 3 - GENERACI√ìN DE EMBEDDINGS SEM√ÅNTICOS DE LAS BRECHAS\n",
    "# ====================================================================\n",
    "# En esta celda se genera la representaci√≥n vectorial (embeddings) de cada brecha\n",
    "# para permitir b√∫squeda por similitud sem√°ntica\n",
    "# ====================================================================\n",
    "\n",
    "# ====================================================================\n",
    "# OPCIONES DE MODELOS DE EMBEDDINGS (elige UNO seg√∫n tus recursos)\n",
    "# ====================================================================\n",
    "\n",
    "# OPCI√ìN 1: MiniLM (R√ÅPIDO, 384 dims) - Recomendado para desarrollo/testing\n",
    "# Ventaja: Bajo consumo de RAM/VRAM, velocidad alta\n",
    "# Desventaja: Menor precisi√≥n sem√°ntica\n",
    "# embed_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# OPCI√ìN 2: MPNet (BALANCEADO, 768 dims) - Recomendado para producci√≥n\n",
    "# Ventaja: Mejor balance precisi√≥n/velocidad\n",
    "# Desventaja: Requiere ~2x m√°s RAM que MiniLM\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# OPCI√ìN 3: Modelos espec√≠ficos para espa√±ol (768 dims)\n",
    "# Ventaja: Entrenados espec√≠ficamente en espa√±ol, mejor comprensi√≥n del idioma\n",
    "# Desventaja: Mayor consumo de recursos\n",
    "# embed_model = SentenceTransformer(\"hiiamsid/sentence_similarity_spanish_es\")\n",
    "# O alternativa:\n",
    "# embed_model = SentenceTransformer(\"hackathon-pln-es/paraphrase-spanish-distilroberta\")\n",
    "\n",
    "# OPCI√ìN 4: Modelo GRANDE (1024 dims) - Solo si tienes GPU potente\n",
    "# Ventaja: M√°xima precisi√≥n sem√°ntica\n",
    "# Desventaja: Requiere GPU con 8GB+ VRAM, muy lento en CPU\n",
    "# embed_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# ====================================================================\n",
    "# COMPARACI√ìN DE MODELOS\n",
    "# ====================================================================\n",
    "# | Modelo                                  | Dims | Velocidad | Precisi√≥n | RAM    |\n",
    "# |-----------------------------------------|------|-----------|-----------|--------|\n",
    "# | paraphrase-multilingual-MiniLM-L12-v2   | 384  | ‚ö°‚ö°‚ö°      | ‚≠ê‚≠ê      | ~500MB |\n",
    "# | paraphrase-multilingual-mpnet-base-v2   | 768  | ‚ö°‚ö°        | ‚≠ê‚≠ê‚≠ê    | ~1GB   |\n",
    "# | hiiamsid/sentence_similarity_spanish_es | 768  | ‚ö°‚ö°        | ‚≠ê‚≠ê‚≠ê‚≠ê  | ~1GB   |\n",
    "# | LaBSE (Google)                          | 768  | ‚ö°         | ‚≠ê‚≠ê‚≠ê‚≠ê  | ~2GB   |\n",
    "# ====================================================================\n",
    "\n",
    "# ====================================================================\n",
    "# CODIFICACI√ìN DE BRECHAS A VECTORES\n",
    "# ====================================================================\n",
    "\n",
    "# Extraer la columna \"brecha\" del DataFrame como lista de strings\n",
    "texts = brechas[\"brecha\"].tolist()\n",
    "\n",
    "# Generar embeddings para cada brecha\n",
    "# - show_progress_bar=True: muestra barra de progreso durante la codificaci√≥n\n",
    "# - convert_to_numpy=True: convierte resultado a array numpy para compatibilidad con FAISS\n",
    "# - normalize_embeddings=True: normaliza vectores a norma L2=1 (facilita c√°lculo de similitud coseno)\n",
    "brecha_embeddings = embed_model.encode(\n",
    "    texts, \n",
    "    show_progress_bar=True, \n",
    "    convert_to_numpy=True, \n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# ====================================================================\n",
    "# GUARDAR EMBEDDINGS Y DATOS PARA USO POSTERIOR\n",
    "# ====================================================================\n",
    "\n",
    "# Guardar los embeddings como archivo .npy (formato binario eficiente de numpy)\n",
    "np.save(\"outputs/brecha_embeddings.npy\", brecha_embeddings)\n",
    "\n",
    "# Guardar el DataFrame de brechas con √≠ndices para referencia posterior\n",
    "# index=False: no incluye la columna de √≠ndice de pandas en el CSV\n",
    "brechas.to_csv(\"outputs/brechas_with_idx.csv\", index=False)\n",
    "\n",
    "# Mostrar las dimensiones de la matriz de embeddings (n√∫mero_brechas x dimensi√≥n_vector)\n",
    "print(\"Embeddings shape:\", brecha_embeddings.shape)\n",
    "print(f\"Modelo usado: {embed_model._model_card_vars.get('model_name', 'N/A')}\")\n",
    "print(f\"Dimensiones por embedding: {brecha_embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f375d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ntotal: 48\n",
      "Dimensi√≥n de vectores: 768\n",
      "Tipo de √≠ndice: Inner Product (similitud coseno en vectores normalizados)\n"
     ]
    }
   ],
   "source": [
    "# CELDA 4 - CREACI√ìN DEL √çNDICE FAISS PARA B√öSQUEDA VECTORIAL R√ÅPIDA\n",
    "# ====================================================================\n",
    "# FAISS (Facebook AI Similarity Search) es una librer√≠a optimizada para\n",
    "# realizar b√∫squedas de vecinos m√°s cercanos (k-NN) en espacios de alta dimensi√≥n\n",
    "# ====================================================================\n",
    "\n",
    "# ====================================================================\n",
    "# CARGA DE EMBEDDINGS DESDE ARCHIVO\n",
    "# ====================================================================\n",
    "\n",
    "# Cargar los embeddings previamente generados en la celda 3\n",
    "# np.load(): Lee el archivo .npy que contiene la matriz de embeddings\n",
    "# astype(\"float32\"): Convierte a float32 (FAISS requiere este tipo de dato para optimizaci√≥n)\n",
    "emb = np.load(\"outputs/brecha_embeddings.npy\").astype(\"float32\")\n",
    "\n",
    "# Obtener la dimensionalidad de los vectores (n√∫mero de columnas)\n",
    "# emb.shape[1] devuelve 384 (MiniLM) o 768 (MPNet), etc.\n",
    "d = emb.shape[1]\n",
    "\n",
    "# ====================================================================\n",
    "# CREACI√ìN DEL √çNDICE FAISS\n",
    "# ====================================================================\n",
    "\n",
    "# IndexFlatIP: √çndice FAISS que usa producto interno (Inner Product)\n",
    "# - \"Flat\" significa b√∫squeda exacta (no aproximada), m√°s preciso pero m√°s lento\n",
    "# - \"IP\" (Inner Product) en vectores normalizados es equivalente a similitud coseno\n",
    "# - d: dimensi√≥n de los vectores (debe coincidir con la dimensi√≥n de los embeddings)\n",
    "index = faiss.IndexFlatIP(d)\n",
    "\n",
    "# Alternativas de √≠ndices FAISS:\n",
    "# - faiss.IndexFlatL2(d): usa distancia L2 (euclidiana) en lugar de producto interno\n",
    "# - faiss.IndexIVFFlat(quantizer, d, nlist): m√°s r√°pido para muchos vectores (b√∫squeda aproximada)\n",
    "# - faiss.IndexHNSWFlat(d, M): √≠ndice basado en grafos, muy r√°pido para alta dimensi√≥n\n",
    "\n",
    "# ====================================================================\n",
    "# AGREGAR VECTORES AL √çNDICE\n",
    "# ====================================================================\n",
    "\n",
    "# A√±adir todos los embeddings de brechas al √≠ndice FAISS\n",
    "# Esto construye la estructura de datos interna para b√∫squedas eficientes\n",
    "index.add(emb)\n",
    "\n",
    "# ====================================================================\n",
    "# GUARDAR EL √çNDICE EN DISCO\n",
    "# ====================================================================\n",
    "\n",
    "# Guardar el √≠ndice FAISS como archivo binario .faiss\n",
    "# Esto permite reutilizar el √≠ndice sin tener que reconstruirlo cada vez\n",
    "faiss.write_index(index, \"outputs/brecha_index.faiss\")\n",
    "\n",
    "# ====================================================================\n",
    "# VERIFICACI√ìN\n",
    "# ====================================================================\n",
    "\n",
    "# Mostrar el n√∫mero total de vectores indexados\n",
    "# index.ntotal debe ser igual al n√∫mero de brechas (6 en tu caso)\n",
    "print(\"Index ntotal:\", index.ntotal)\n",
    "print(f\"Dimensi√≥n de vectores: {d}\")\n",
    "print(f\"Tipo de √≠ndice: Inner Product (similitud coseno en vectores normalizados)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec9ef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape labels: (817, 48)\n"
     ]
    }
   ],
   "source": [
    "# CELDA 5 - PREPARACI√ìN DE ETIQUETAS MULTIETIQUETA PARA CLASIFICACI√ìN\n",
    "# ====================================================================\n",
    "# Objetivo: Convertir las cadenas de ids de brecha (\"1,3,5\") asociadas a cada\n",
    "# proyecto en un formato matricial binario (one-hot multi-label) para entrenamiento.\n",
    "# ====================================================================\n",
    "\n",
    "# Estructura esperada de la columna proyectos[\"brecha_ids\"]:\n",
    "#   - Cada fila contiene una cadena con IDs separados por coma (ej: \"2,4\" o \"1\")\n",
    "#   - Puede haber espacios o strings vac√≠os; se limpian en el proceso\n",
    "# Resultado deseado (proyectos[\"brecha_ids_list\"]): lista de ints por fila\n",
    "#   Ej: \"2,4\" -> [2,4]; \"1\" -> [1]; \"\" -> []\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Paso 1: Normalizar y convertir las cadenas en listas de enteros\n",
    "# --------------------------------------------------------------------\n",
    "# .astype(str): asegura que todos los valores sean string (evita errores si hay NaN)\n",
    "# .apply(lambda ...): procesa cada string:\n",
    "#   - s.split(\",\") separa por comas -> [\"2\",\"4\"]\n",
    "#   - x.strip() elimina espacios\n",
    "#   - if x.strip() != '' filtra vac√≠os (por ejemplo si hab√≠a \",,\")\n",
    "#   - int(x) convierte cada fragmento a entero\n",
    "proyectos[\"brecha_ids_list\"] = proyectos[\"brecha_ids\"].astype(str).apply(\n",
    "    lambda s: [int(x) for x in s.split(\",\") if x.strip() != '']\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Paso 2: Inicializar MultiLabelBinarizer con el universo ordenado de clases\n",
    "# --------------------------------------------------------------------\n",
    "# MultiLabelBinarizer transforma listas de labels en una matriz binaria:\n",
    "#   Ejemplo:\n",
    "#       clases = [1,2,3,4]\n",
    "#       entrada: [1,3] -> [1,0,1,0]\n",
    "# sorted(brechas[\"id\"].tolist()) asegura orden consistente de columnas.\n",
    "mlb = MultiLabelBinarizer(classes=sorted(brechas[\"id\"].tolist()))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Paso 3: Fit + Transform\n",
    "# --------------------------------------------------------------------\n",
    "# mlb.fit_transform(lista_de_listas) genera la matriz Y:\n",
    "#   - Filas: proyectos\n",
    "#   - Columnas: cada brecha (en el orden de mlb.classes_)\n",
    "#   - Valores: 1 si la brecha est√° asociada al proyecto, 0 si no.\n",
    "Y = mlb.fit_transform(proyectos[\"brecha_ids_list\"])\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Paso 4: Inspecci√≥n r√°pida\n",
    "# --------------------------------------------------------------------\n",
    "# Forma esperada: (n_proyectos, n_brechas)\n",
    "print(\"Shape labels:\", Y.shape)\n",
    "\n",
    "# Informaci√≥n adicional √∫til (descomentar si necesitas ver m√°s detalles):\n",
    "# print(\"Clases (orden):\", mlb.classes_)\n",
    "# print(\"Ejemplo primera fila lista original:\", proyectos[\"brecha_ids_list\"].iloc[0])\n",
    "# print(\"Vector binario primera fila:\", Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be3a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset total: 817 proyectos\n",
      "Train: 694 proyectos (84.9%)\n",
      "Validaci√≥n: 123 proyectos (15.1%)\n"
     ]
    }
   ],
   "source": [
    "# CELDA 6 - PREPARACI√ìN DE DATASET PARA ENTRENAMIENTO DE BERT\n",
    "# ====================================================================\n",
    "# Objetivo: Crear un Dataset de PyTorch que tokeniza los textos de proyectos\n",
    "# y los asocia con sus etiquetas multi-label para entrenar el modelo BETO (BERT espa√±ol)\n",
    "# ====================================================================\n",
    "\n",
    "# ====================================================================\n",
    "# CARGA DEL TOKENIZADOR\n",
    "# ====================================================================\n",
    "\n",
    "# Cargar el tokenizador del modelo BETO (BERT pre-entrenado en espa√±ol)\n",
    "# \"dccuchile/bert-base-spanish-wwm-cased\":\n",
    "#   - Modelo BERT entrenado en espa√±ol por la Universidad de Chile\n",
    "#   - \"wwm\" = Whole Word Masking (enmascara palabras completas durante pre-entrenamiento)\n",
    "#   - \"cased\" = distingue may√∫sculas/min√∫sculas (importante para nombres propios)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "# ====================================================================\n",
    "# DEFINICI√ìN DE LA CLASE DATASET PERSONALIZADA\n",
    "# ====================================================================\n",
    "\n",
    "class BrechaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para clasificaci√≥n multi-label de proyectos en brechas.\n",
    "    \n",
    "    Hereda de torch.utils.data.Dataset para integrarse con DataLoader de PyTorch.\n",
    "    Cada elemento del dataset contiene:\n",
    "    - Texto tokenizado del proyecto (input_ids, attention_mask, etc.)\n",
    "    - Vector binario de etiquetas (labels) indicando qu√© brechas aplican\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): Lista de strings con los textos de proyectos\n",
    "            labels (numpy.ndarray): Matriz binaria (n_proyectos x n_brechas) de etiquetas\n",
    "            tokenizer: Tokenizador de HuggingFace para convertir texto a tokens\n",
    "            max_len (int): Longitud m√°xima de secuencia (tokens m√°s all√° se truncan)\n",
    "        \"\"\"\n",
    "        self.texts = texts          # Textos originales de los proyectos\n",
    "        self.labels = labels        # Matriz Y de etiquetas binarias\n",
    "        self.tokenizer = tokenizer  # Tokenizador BETO\n",
    "        self.max_len = max_len      # Longitud m√°xima permitida (256 tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retorna el n√∫mero total de ejemplos en el dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Obtiene un ejemplo del dataset en el √≠ndice dado.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): √çndice del ejemplo a recuperar\n",
    "            \n",
    "        Returns:\n",
    "            dict: Diccionario con:\n",
    "                - 'input_ids': IDs de tokens del texto\n",
    "                - 'attention_mask': M√°scara de atenci√≥n (1 para tokens reales, 0 para padding)\n",
    "                - 'token_type_ids': IDs de tipo de token (para modelos BERT)\n",
    "                - 'labels': Vector binario de etiquetas (float para BCEWithLogitsLoss)\n",
    "        \"\"\"\n",
    "        # Convertir el texto a string (por si hay valores NaN o num√©ricos)\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # Tokenizar el texto usando el tokenizador BETO\n",
    "        # - truncation=True: corta el texto si excede max_len\n",
    "        # - padding='max_length': rellena con tokens especiales hasta max_len\n",
    "        # - max_length=self.max_len: longitud objetivo (256)\n",
    "        # - return_tensors='pt': retorna tensores de PyTorch\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_len, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Convertir tensores de forma (1, seq_len) a (seq_len,)\n",
    "        # squeeze(0) elimina la dimensi√≥n batch a√±adida por return_tensors='pt'\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        \n",
    "        # Agregar las etiquetas como tensor float (requerido para p√©rdida BCEWithLogits)\n",
    "        # labels[idx] es un array numpy de 0s y 1s indicando presencia de cada brecha\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# ====================================================================\n",
    "# PREPARACI√ìN DE TEXTOS DE ENTRADA\n",
    "# ====================================================================\n",
    "\n",
    "# Concatenar t√≠tulo y descripci√≥n de cada proyecto con un punto separador\n",
    "# .fillna(\"\") reemplaza valores NaN con string vac√≠o para evitar errores\n",
    "# Formato resultante: \"TITULO DEL PROYECTO. Descripci√≥n detallada del proyecto...\"\n",
    "texts = (proyectos[\"title\"].fillna(\"\") + \". \" + proyectos[\"description\"].fillna(\"\")).tolist()\n",
    "\n",
    "# ====================================================================\n",
    "# CREACI√ìN DEL DATASET COMPLETO\n",
    "# ====================================================================\n",
    "\n",
    "# Instanciar el dataset con todos los proyectos y sus etiquetas\n",
    "dataset = BrechaDataset(texts, Y, tokenizer)\n",
    "\n",
    "# ====================================================================\n",
    "# DIVISI√ìN TRAIN/VALIDATION\n",
    "# ====================================================================\n",
    "\n",
    "# Importar funci√≥n para dividir datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir √≠ndices en conjuntos de entrenamiento (85%) y validaci√≥n (15%)\n",
    "# - range(len(dataset)): genera √≠ndices [0, 1, 2, ..., n-1]\n",
    "# - test_size=0.15: 15% de datos para validaci√≥n\n",
    "# - random_state=42: semilla aleatoria para reproducibilidad\n",
    "idx_train, idx_val = train_test_split(\n",
    "    range(len(dataset)), \n",
    "    test_size=0.15, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ====================================================================\n",
    "# CREACI√ìN DE SUBSETS PARA ENTRENAMIENTO Y VALIDACI√ìN\n",
    "# ====================================================================\n",
    "\n",
    "# Importar Subset para crear vistas del dataset original\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Crear subset de entrenamiento con √≠ndices seleccionados\n",
    "train_dataset = Subset(dataset, idx_train)\n",
    "\n",
    "# Crear subset de validaci√≥n con √≠ndices seleccionados\n",
    "val_dataset = Subset(dataset, idx_val)\n",
    "\n",
    "# Resumen de datasets creados\n",
    "print(f\"Dataset total: {len(dataset)} proyectos\")\n",
    "print(f\"Train: {len(train_dataset)} proyectos ({len(train_dataset)/len(dataset)*100:.1f}%)\")\n",
    "print(f\"Validaci√≥n: {len(val_dataset)} proyectos ({len(val_dataset)/len(dataset)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17faad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 87/261 [00:26<00:43,  3.99it/s]\n",
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19434063136577606, 'eval_f1_micro': 0.0, 'eval_precision_micro': 0.0, 'eval_recall_micro': 0.0, 'eval_runtime': 1.1849, 'eval_samples_per_second': 103.803, 'eval_steps_per_second': 6.751, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "                                                 \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 174/261 [00:52<00:19,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13839370012283325, 'eval_f1_micro': 0.0, 'eval_precision_micro': 0.0, 'eval_recall_micro': 0.0, 'eval_runtime': 1.085, 'eval_samples_per_second': 113.364, 'eval_steps_per_second': 7.373, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "                                                 \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 261/261 [01:18<00:00,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1277291178703308, 'eval_f1_micro': 0.0, 'eval_precision_micro': 0.0, 'eval_recall_micro': 0.0, 'eval_runtime': 1.0182, 'eval_samples_per_second': 120.801, 'eval_steps_per_second': 7.857, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 261/261 [01:20<00:00,  3.26it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 80.0197, 'train_samples_per_second': 26.019, 'train_steps_per_second': 3.262, 'train_loss': 0.20717234995173311, 'epoch': 3.0}\n",
      "\n",
      "============================================================\n",
      "‚úÖ ENTRENAMIENTO COMPLETADO\n",
      "============================================================\n",
      "Modelo guardado en: models/beto_brechas_best\n",
      "Checkpoints intermedios en: models/beto_brechas/checkpoint-*\n",
      "Logs de entrenamiento en: logs/\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "‚úÖ ENTRENAMIENTO COMPLETADO\n",
      "============================================================\n",
      "Modelo guardado en: models/beto_brechas_best\n",
      "Checkpoints intermedios en: models/beto_brechas/checkpoint-*\n",
      "Logs de entrenamiento en: logs/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CELDA 7 - ENTRENAMIENTO DEL MODELO BERT PARA CLASIFICACI√ìN MULTI-LABEL\n",
    "# ====================================================================\n",
    "# Objetivo: Entrenar el modelo BETO (BERT espa√±ol) para clasificar proyectos\n",
    "# en m√∫ltiples brechas simult√°neamente usando los datasets preparados en CELDA 6\n",
    "# ====================================================================\n",
    "\n",
    "# ====================================================================\n",
    "# CARGA Y CONFIGURACI√ìN DEL MODELO BASE\n",
    "# ====================================================================\n",
    "\n",
    "# Cargar modelo BERT pre-entrenado y configurarlo para clasificaci√≥n multi-label\n",
    "# AutoModelForSequenceClassification.from_pretrained():\n",
    "#   - \"dccuchile/bert-base-spanish-wwm-cased\": modelo BETO pre-entrenado\n",
    "#   - problem_type=\"multi_label_classification\": indica que es multi-label (no multi-class)\n",
    "#     Esto hace que internamente use BCEWithLogitsLoss en lugar de CrossEntropyLoss\n",
    "#   - num_labels=Y.shape[1]: n√∫mero de clases (6 brechas en tu caso)\n",
    "#     Determina el tama√±o de la capa de salida del clasificador\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "    problem_type=\"multi_label_classification\",  # Multi-label: cada proyecto puede tener m√∫ltiples brechas\n",
    "    num_labels=Y.shape[1]  # N√∫mero de brechas distintas (columnas de la matriz Y)\n",
    ")\n",
    "\n",
    "# ====================================================================\n",
    "# CONFIGURACI√ìN DE HIPERPAR√ÅMETROS DE ENTRENAMIENTO\n",
    "# ====================================================================\n",
    "\n",
    "# TrainingArguments: Clase que encapsula todos los hiperpar√°metros del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    # Directorio donde se guardar√°n los checkpoints del modelo durante entrenamiento\n",
    "    output_dir=\"models/beto_brechas\",\n",
    "    \n",
    "    # Tama√±o del batch por dispositivo (GPU/CPU) durante entrenamiento\n",
    "    # 8 proyectos procesados simult√°neamente (ajustar seg√∫n VRAM disponible)\n",
    "    per_device_train_batch_size=8,\n",
    "    \n",
    "    # Tama√±o del batch durante evaluaci√≥n (puede ser mayor porque no usa gradientes)\n",
    "    # 16 proyectos procesados simult√°neamente en validaci√≥n\n",
    "    per_device_eval_batch_size=16,\n",
    "    \n",
    "    # Estrategia de evaluaci√≥n: \"epoch\" = evaluar al final de cada √©poca\n",
    "    # Alternativas: \"steps\" (cada N pasos), \"no\" (sin evaluaci√≥n)\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    \n",
    "    # Estrategia de guardado: \"epoch\" = guardar checkpoint al final de cada √©poca\n",
    "    # Crear√° subcarpetas checkpoint-1, checkpoint-2, checkpoint-3\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # N√∫mero de √©pocas de entrenamiento (pasadas completas por todo el dataset)\n",
    "    # 3 √©pocas es t√≠pico para fine-tuning de BERT\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Tasa de aprendizaje (learning rate) para el optimizador Adam\n",
    "    # 2e-5 (0.00002) es el valor est√°ndar recomendado para fine-tuning de BERT\n",
    "    learning_rate=2e-5,\n",
    "    \n",
    "    # Directorio donde se guardar√°n los logs de TensorBoard\n",
    "    # √ötil para visualizar p√©rdida, m√©tricas, etc. durante entrenamiento\n",
    "    logging_dir=\"logs\",\n",
    "    \n",
    "    # Cargar el mejor modelo al finalizar el entrenamiento (seg√∫n metric_for_best_model)\n",
    "    # True = al final, el modelo en memoria ser√° el mejor checkpoint, no el √∫ltimo\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # M√©trica para determinar cu√°l es el \"mejor\" modelo\n",
    "    # \"eval_loss\" = menor p√©rdida en validaci√≥n (alternativas: \"eval_f1_micro\", etc.)\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "# ====================================================================\n",
    "# FUNCI√ìN DE C√ÅLCULO DE M√âTRICAS PARA EVALUACI√ìN\n",
    "# ====================================================================\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas de clasificaci√≥n multi-label durante evaluaci√≥n.\n",
    "    \n",
    "    Esta funci√≥n es llamada autom√°ticamente por Trainer al final de cada √©poca\n",
    "    para evaluar el rendimiento del modelo en el conjunto de validaci√≥n.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred (tuple): Tupla (logits, labels) donde:\n",
    "            - logits: salidas crudas del modelo (antes de sigmoid), shape (n_samples, n_labels)\n",
    "            - labels: etiquetas verdaderas binarias, shape (n_samples, n_labels)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con m√©tricas calculadas:\n",
    "            - f1_micro: F1-score micro-promediado (trata todas las predicciones como un conjunto)\n",
    "            - precision_micro: Precisi√≥n micro-promediada\n",
    "            - recall_micro: Recall micro-promediado\n",
    "    \"\"\"\n",
    "    # Desempaquetar predicciones y etiquetas verdaderas\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Importar numpy localmente (ya est√° importado globalmente, pero por claridad)\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convertir logits a probabilidades usando funci√≥n sigmoide\n",
    "    # sigmoid(x) = 1 / (1 + e^(-x))\n",
    "    # Transforma logits (-‚àû, +‚àû) a probabilidades [0, 1]\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    \n",
    "    # Convertir probabilidades a predicciones binarias usando umbral 0.5\n",
    "    # prob >= 0.5 -> 1 (brecha asignada)\n",
    "    # prob < 0.5 -> 0 (brecha no asignada)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    \n",
    "    # Importar m√©tricas de scikit-learn\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    \n",
    "    # Calcular y retornar m√©tricas:\n",
    "    # - average='micro': calcula m√©tricas globalmente contando totales de TP, FP, FN\n",
    "    #   (trata todo como una clasificaci√≥n binaria grande)\n",
    "    # - zero_division=0: si hay divisi√≥n por cero (no hay predicciones), retornar 0\n",
    "    return {\n",
    "        # F1-score: media arm√≥nica de precisi√≥n y recall (2*P*R / (P+R))\n",
    "        \"f1_micro\": f1_score(labels, preds, average='micro', zero_division=0),\n",
    "        \n",
    "        # Precisi√≥n: TP / (TP + FP) - de las brechas predichas, cu√°ntas son correctas\n",
    "        \"precision_micro\": precision_score(labels, preds, average='micro', zero_division=0),\n",
    "        \n",
    "        # Recall: TP / (TP + FN) - de las brechas verdaderas, cu√°ntas fueron detectadas\n",
    "        \"recall_micro\": recall_score(labels, preds, average='micro', zero_division=0),\n",
    "    }\n",
    "\n",
    "# ====================================================================\n",
    "# INICIALIZACI√ìN DEL ENTRENADOR (TRAINER)\n",
    "# ====================================================================\n",
    "\n",
    "# Trainer: Clase de alto nivel de HuggingFace que encapsula el loop de entrenamiento\n",
    "# Maneja autom√°ticamente: forward pass, backward pass, optimizaci√≥n, evaluaci√≥n, logging, checkpoints\n",
    "trainer = Trainer(\n",
    "    model=model,                    # Modelo BETO a entrenar\n",
    "    args=training_args,             # Configuraci√≥n de hiperpar√°metros\n",
    "    train_dataset=train_dataset,    # Dataset de entrenamiento (85% de datos)\n",
    "    eval_dataset=val_dataset,       # Dataset de validaci√≥n (15% de datos)\n",
    "    compute_metrics=compute_metrics # Funci√≥n para calcular m√©tricas en evaluaci√≥n\n",
    ")\n",
    "\n",
    "# ====================================================================\n",
    "# EJECUCI√ìN DEL ENTRENAMIENTO\n",
    "# ====================================================================\n",
    "\n",
    "# Iniciar el proceso de entrenamiento\n",
    "# Esto ejecutar√°:\n",
    "#   1. num_train_epochs (3) √©pocas de entrenamiento\n",
    "#   2. Evaluaci√≥n al final de cada √©poca (evaluation_strategy=\"epoch\")\n",
    "#   3. Guardado de checkpoints al final de cada √©poca (save_strategy=\"epoch\")\n",
    "#   4. C√°lculo de m√©tricas (F1, precisi√≥n, recall) en cada evaluaci√≥n\n",
    "#   5. Selecci√≥n del mejor modelo seg√∫n eval_loss (load_best_model_at_end=True)\n",
    "# \n",
    "# NOTA: Este proceso puede tardar varios minutos u horas seg√∫n:\n",
    "#   - Tama√±o del dataset (n√∫mero de proyectos)\n",
    "#   - Hardware disponible (GPU vs CPU)\n",
    "#   - Hiperpar√°metros (batch_size, num_epochs)\n",
    "# \n",
    "# Progreso mostrado: barra de progreso con loss, learning rate, samples/sec\n",
    "trainer.train()\n",
    "\n",
    "# ====================================================================\n",
    "# GUARDADO DEL MEJOR MODELO ENTRENADO\n",
    "# ====================================================================\n",
    "\n",
    "# Guardar el mejor modelo (seg√∫n eval_loss) en disco\n",
    "# Se guardar√°n:\n",
    "#   - config.json: configuraci√≥n del modelo\n",
    "#   - pytorch_model.bin: pesos del modelo entrenado\n",
    "#   - tokenizer_config.json, vocab.txt, etc.: archivos del tokenizador\n",
    "# \n",
    "# Este modelo puede ser recargado m√°s tarde con:\n",
    "# AutoModelForSequenceClassification.from_pretrained(\"models/beto_brechas_best\")\n",
    "trainer.save_model(\"models/beto_brechas_best\")\n",
    "\n",
    "# Guardar tambi√©n el tokenizador en el mismo directorio\n",
    "# Esto permite cargar todo junto en la CELDA 8 sin errores\n",
    "tokenizer.save_pretrained(\"models/beto_brechas_best\")\n",
    "\n",
    "# ====================================================================\n",
    "# VERIFICACI√ìN POST-ENTRENAMIENTO\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Modelo guardado en: models/beto_brechas_best\")\n",
    "print(f\"Checkpoints intermedios en: models/beto_brechas/checkpoint-*\")\n",
    "print(f\"Logs de entrenamiento en: logs/\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3bef90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRUEBA DE CLASIFICACI√ìN H√çBRIDA - PROYECTO REAL DEL DATASET\n",
      "================================================================================\n",
      "Project ID: 211\n",
      "T√≠tulo: MEJORAMIENTO DEL SERVICIO DE EDUCACION INICIAL EN 03 I.E.I., DISTRITO DE SAN PEDRO DE HUACARPANA - CHINCHA - ICA\n",
      "\n",
      "BRECHA VERDADERA: 22\n",
      "Descripci√≥n brecha: PORCENTAJE DE LOCALES EDUCATIVOS CON EL SERVICIO DE EDUCACI√ìN INICIAL CON CAPACIDAD INSTALADA INADECUADA\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Resultados de clasificaci√≥n (top 5 candidatos):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Brecha ID 23: PORCENTAJE DE LOCALES EDUCATIVOS CON EL SERVICIO DE EDUCACI√ìN PRIMARIA CON CAPAC... \n",
      "   - BERT score:       0.1189\n",
      "   - Embedding score:  0.7596\n",
      "   - FINAL score:      0.3752\n",
      "\n",
      "2. Brecha ID 7: PORCENTAJE DE CENTROS DE EDUCACI√ìN B√ÅSICA ESPECIAL CON CAPACIDAD INSTALADA INADE... \n",
      "   - BERT score:       0.0802\n",
      "   - Embedding score:  0.7679\n",
      "   - FINAL score:      0.3553\n",
      "\n",
      "3. Brecha ID 22: PORCENTAJE DE LOCALES EDUCATIVOS CON EL SERVICIO DE EDUCACI√ìN INICIAL CON CAPACI... ‚úÖ CORRECTO\n",
      "   - BERT score:       0.0729\n",
      "   - Embedding score:  0.7741\n",
      "   - FINAL score:      0.3533\n",
      "\n",
      "4. Brecha ID 24: PORCENTAJE DE LOCALES EDUCATIVOS CON EL SERVICIO DE EDUCACI√ìN SECUNDARIA CON CAP... \n",
      "   - BERT score:       0.0703\n",
      "   - Embedding score:  0.7532\n",
      "   - FINAL score:      0.3435\n",
      "\n",
      "5. Brecha ID 26: PORCENTAJE DE LOCALES EDUCATIVOS CON SERVICIO DEL PROGRAMA DE INTERVENCI√ìN TEMPR... \n",
      "   - BERT score:       0.0933\n",
      "   - Embedding score:  0.7160\n",
      "   - FINAL score:      0.3424\n",
      "\n",
      "================================================================================\n",
      "‚ùå PREDICCI√ìN INCORRECTA\n",
      "   Predijo: Brecha ID 23\n",
      "   Esperado: Brecha ID 22\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PRUEBA CON PROYECTO DE EJEMPLO (CUNA JARDIN)\n",
      "================================================================================\n",
      "Proyecto: AMPLIACION DE 01 AULA + 01 DIRECCION EN LA CUNA JARDIN, HOSPITAL AMAZONICO- PUERTO CALLAO\n",
      "\n",
      "‚ö†Ô∏è NOTA: Este proyecto menciona 'CUNA JARDIN' (educaci√≥n inicial)\n",
      "Las brechas relacionadas en el dataset son:\n",
      "  - Brecha 5: CENTROS CUNA M√ÅS EN CONDICIONES INADECUADAS\n",
      "  - Brecha 22: LOCALES EDUCATIVOS CON SERVICIO DE EDUCACI√ìN INICIAL INADECUADA\n",
      "  - Brecha 48: PERSONAS NO MATRICULADAS EN NIVEL INICIAL (‚ö†Ô∏è SIN DATOS DE ENTRENAMIENTO)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Resultados (top 5 candidatos):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Brecha ID 13: PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN CONDICIONES INADECUAD...\n",
      "   - BERT score:       0.0839\n",
      "   - Embedding score:  0.6849\n",
      "   - FINAL score:      0.3243\n",
      "\n",
      "2. Brecha ID 1: INDICADOR DE BRECHA POR DEFINIR...\n",
      "   - BERT score:       0.0986\n",
      "   - Embedding score:  0.6521\n",
      "   - FINAL score:      0.3200\n",
      "\n",
      "3. Brecha ID 5: PORCENTAJE DE CENTROS CUNA M√ÅS EN CONDICIONES INADECUADAS...\n",
      "   - BERT score:       0.0641\n",
      "   - Embedding score:  0.6874\n",
      "   - FINAL score:      0.3134\n",
      "\n",
      "4. Brecha ID 12: PORCENTAJE DE HORAS AL D√çA SIN SERVICIO DE AGUA POTABLE EN EL √ÅMBITO URBANO...\n",
      "   - BERT score:       0.0739\n",
      "   - Embedding score:  0.6371\n",
      "   - FINAL score:      0.2992\n",
      "\n",
      "5. Brecha ID 4: PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA...\n",
      "   - BERT score:       0.0658\n",
      "   - Embedding score:  0.6013\n",
      "   - FINAL score:      0.2800\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Predicci√≥n principal: Brecha ID 13\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS DE DISTRIBUCI√ìN DE BRECHAS EN EL DATASET DE ENTRENAMIENTO\n",
      "================================================================================\n",
      "\n",
      "Total de proyectos: 817\n",
      "Total de brechas en cat√°logo: 48\n",
      "Brechas con al menos 1 proyecto: 47\n",
      "Brechas sin proyectos: 1\n",
      "\n",
      "üìä Top 10 brechas m√°s frecuentes:\n",
      "1. Brecha 18: 151 proyectos - PORCENTAJE DE LA POBLACI√ìN URBANA SIN ACCESO A LOS SERVICIOS...\n",
      "2. Brecha 16: 112 proyectos - PORCENTAJE DE LA POBLACI√ìN RURAL SIN ACCESO AL SERVICIO DE A...\n",
      "3. Brecha 17: 91 proyectos - PORCENTAJE DE LA POBLACI√ìN RURAL SIN ACCESO AL SERVICIO DE A...\n",
      "4. Brecha 30: 66 proyectos - PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS PARA EL ESPARCIMIENTO ...\n",
      "5. Brecha 38: 51 proyectos - PORCENTAJE DE PRODUCTORES AGROPECUARIOS SIN SERVICIO DE ASIS...\n",
      "6. Brecha 23: 46 proyectos - PORCENTAJE DE LOCALES EDUCATIVOS CON EL SERVICIO DE EDUCACI√ì...\n",
      "7. Brecha 19: 29 proyectos - PORCENTAJE DE LA POBLACI√ìN URBANA SIN ACCESO A SERVICIOS DE ...\n",
      "8. Brecha 20: 28 proyectos - PORCENTAJE DE LA POBLACI√ìN URBANA SIN ACCESO AL SERVICIO DE ...\n",
      "9. Brecha 13: 28 proyectos - PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN C...\n",
      "10. Brecha 22: 27 proyectos - PORCENTAJE DE LOCALES EDUCATIVOS CON EL SERVICIO DE EDUCACI√ì...\n",
      "\n",
      "‚ö†Ô∏è Brechas sin ejemplos de entrenamiento:\n",
      "  - Brecha 48: PORCENTAJE DE PERSONAS NO MATRICULADAS EN EL NIVEL INICIAL RESPECTO A ...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CELDA 8 - CLASIFICACI√ìN H√çBRIDA: COMBINACI√ìN DE B√öSQUEDA SEM√ÅNTICA + BERT\n",
    "# ====================================================================\n",
    "# Objetivo: Implementar un sistema h√≠brido que combine la b√∫squeda vectorial (FAISS)\n",
    "# con la clasificaci√≥n BERT para obtener mejores predicciones que cada m√©todo por separado\n",
    "# ====================================================================\n",
    "\n",
    "# ====================================================================\n",
    "# CARGA DE RECURSOS NECESARIOS PARA INFERENCIA\n",
    "# ====================================================================\n",
    "\n",
    "# Cargar √≠ndice FAISS con embeddings de brechas (creado en CELDA 4)\n",
    "# Este √≠ndice permite b√∫squeda r√°pida de las brechas m√°s similares sem√°nticamente\n",
    "index = faiss.read_index(\"outputs/brecha_index.faiss\")\n",
    "\n",
    "# Cargar DataFrame de brechas con sus IDs y textos\n",
    "# Necesario para mapear los √≠ndices de FAISS a brechas legibles\n",
    "brechas = pd.read_csv(\"outputs/brechas_with_idx.csv\")\n",
    "\n",
    "# Cargar modelo de embeddings para convertir el texto del proyecto en vector\n",
    "# IMPORTANTE: Debe ser el MISMO modelo usado en CELDA 3 para mantener consistencia\n",
    "# Si cambiaste a MPNet en CELDA 3, c√°mbialo aqu√≠ tambi√©n\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# Cargar tokenizador del modelo BERT entrenado\n",
    "# Convierte texto a tokens compatibles con el modelo BETO entrenado\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"models/beto_brechas_best\")\n",
    "\n",
    "# Cargar modelo BERT entrenado (fine-tuned) para clasificaci√≥n multi-label\n",
    "# Este modelo fue guardado en CELDA 7 despu√©s del entrenamiento\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"models/beto_brechas_best\")\n",
    "\n",
    "# Poner el modelo en modo evaluaci√≥n (desactiva dropout, batch norm, etc.)\n",
    "# CR√çTICO: En modo eval(), el modelo no actualiza pesos y da resultados determin√≠sticos\n",
    "bert_model.eval()\n",
    "\n",
    "# ====================================================================\n",
    "# FUNCI√ìN PRINCIPAL DE CLASIFICACI√ìN H√çBRIDA\n",
    "# ====================================================================\n",
    "\n",
    "def hybrid_classify(project_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Clasifica un proyecto usando enfoque h√≠brido: b√∫squeda vectorial + BERT.\n",
    "    \n",
    "    Estrategia de dos etapas:\n",
    "    1. RECUPERACI√ìN (Retrieval): Usar embeddings + FAISS para encontrar top_k candidatos\n",
    "    2. RE-RANKING: Usar BERT para puntuar candidatos y combinar scores\n",
    "    \n",
    "    Esta arquitectura es m√°s eficiente que evaluar BERT en todas las brechas,\n",
    "    y m√°s precisa que usar solo b√∫squeda vectorial.\n",
    "    \n",
    "    Args:\n",
    "        project_text (str): Texto del proyecto a clasificar (t√≠tulo + descripci√≥n)\n",
    "        top_k (int): N√∫mero de candidatos a recuperar en la fase 1 (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: Lista ordenada de brechas candidatas con scores, formato:\n",
    "            [\n",
    "                {\n",
    "                    \"brecha_id\": int,           # ID de la brecha\n",
    "                    \"brecha_text\": str,         # Texto descriptivo de la brecha\n",
    "                    \"bert_score\": float,        # Probabilidad seg√∫n BERT [0-1]\n",
    "                    \"embed_score\": float,       # Similitud coseno seg√∫n embeddings [0-1]\n",
    "                    \"final_score\": float        # Puntaje combinado (alpha*BERT + beta*embed)\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "            Ordenado de mayor a menor final_score\n",
    "    \"\"\"\n",
    "    \n",
    "    # ====================================================================\n",
    "    # FASE 1: RECUPERACI√ìN DE CANDIDATOS CON B√öSQUEDA VECTORIAL\n",
    "    # ====================================================================\n",
    "    \n",
    "    # Convertir el texto del proyecto a vector usando el mismo modelo de embeddings\n",
    "    # - [project_text]: lista de un elemento (el modelo espera lista)\n",
    "    # - normalize_embeddings=True: normalizar a norma L2=1 (igual que en CELDA 3)\n",
    "    # - astype(\"float32\"): FAISS requiere float32 para b√∫squeda eficiente\n",
    "    qv = embed_model.encode([project_text], normalize_embeddings=True).astype(\"float32\")\n",
    "    \n",
    "    # Buscar las top_k brechas m√°s similares en el √≠ndice FAISS\n",
    "    # index.search(query_vectors, k) retorna:\n",
    "    #   - scores: distancias/similitudes (producto interno en este caso)\n",
    "    #   - idxs: √≠ndices de los vectores m√°s cercanos en el √≠ndice\n",
    "    # Forma: scores y idxs son arrays de shape (1, top_k)\n",
    "    scores, idxs = index.search(qv, top_k)\n",
    "    \n",
    "    # Extraer los √≠ndices como lista Python (idxs[0] porque query tiene batch_size=1)\n",
    "    # candidate_idxs: [3, 1, 5, 2, 4] por ejemplo (√≠ndices en el DataFrame brechas)\n",
    "    candidate_idxs = idxs[0].tolist()\n",
    "\n",
    "    # ====================================================================\n",
    "    # FASE 2: SCORING CON BERT SOBRE TODAS LAS BRECHAS\n",
    "    # ====================================================================\n",
    "    \n",
    "    # Tokenizar el texto del proyecto para BERT\n",
    "    # - return_tensors=\"pt\": retornar tensores de PyTorch\n",
    "    # - truncation=True: cortar si excede max_length\n",
    "    # - padding=True: rellenar hasta max_length\n",
    "    # - max_length=256: misma longitud usada en entrenamiento\n",
    "    inputs = bert_tokenizer(project_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    \n",
    "    # Ejecutar modelo BERT en modo inferencia (sin calcular gradientes)\n",
    "    # torch.no_grad(): desactiva c√°lculo de gradientes (ahorra memoria y acelera)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass del modelo BERT\n",
    "        # .logits: salidas crudas antes de sigmoid, shape (1, n_labels)\n",
    "        # .cpu().numpy(): mover tensor a CPU y convertir a numpy array\n",
    "        # [0]: extraer el primer elemento (batch size = 1)\n",
    "        logits = bert_model(**inputs).logits.cpu().numpy()[0]\n",
    "        \n",
    "        # Convertir logits a probabilidades usando funci√≥n sigmoide\n",
    "        # sigmoid(x) = 1 / (1 + e^(-x))\n",
    "        # probs[i] = probabilidad de que la brecha i aplique al proyecto\n",
    "        # Valores entre [0, 1] donde > 0.5 t√≠picamente significa \"brecha asignada\"\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "    # ====================================================================\n",
    "    # FASE 3: COMBINACI√ìN DE SCORES Y RE-RANKING\n",
    "    # ====================================================================\n",
    "    \n",
    "    # Lista para almacenar resultados con scores combinados\n",
    "    results = []\n",
    "    \n",
    "    # Iterar sobre cada candidato recuperado en la Fase 1\n",
    "    for rank, cand in enumerate(candidate_idxs):\n",
    "        # Obtener fila completa de la brecha desde el DataFrame\n",
    "        # cand es el √≠ndice (0-based) en el DataFrame brechas\n",
    "        brecha_row = brechas.iloc[cand]\n",
    "        \n",
    "        # Calcular √≠ndice en el espacio de labels del modelo BERT\n",
    "        # Si los IDs de brechas empiezan en 1, restar 1 para obtener √≠ndice 0-based\n",
    "        # Ejemplo: brecha_id=1 -> label_index=0, brecha_id=6 -> label_index=5\n",
    "        label_index = int(brecha_row[\"id\"]) - 1\n",
    "        \n",
    "        # Extraer el score BERT para esta brecha espec√≠fica\n",
    "        # probs[label_index]: probabilidad que BERT asigna a esta brecha\n",
    "        bert_score = float(probs[label_index])\n",
    "        \n",
    "        # Extraer el score de similitud sem√°ntica (embeddings)\n",
    "        # scores[0][rank]: similitud coseno (producto interno de vectores normalizados)\n",
    "        # Valores cercanos a 1 = muy similar, cercanos a 0 = poco similar\n",
    "        embed_score = float(scores[0][rank])\n",
    "        \n",
    "        # Combinar ambos scores usando promedio ponderado\n",
    "        # alpha: peso para BERT (√©nfasis en clasificaci√≥n supervisada)\n",
    "        # beta: peso para embeddings (√©nfasis en similitud sem√°ntica)\n",
    "        # alpha + beta debe sumar 1.0 idealmente\n",
    "        alpha, beta = 0.6, 0.4  # 60% BERT, 40% embeddings\n",
    "        \n",
    "        # AJUSTE DE HIPERPAR√ÅMETROS alpha/beta:\n",
    "        # - Aumentar alpha si BERT tiene buen rendimiento en validaci√≥n\n",
    "        # - Aumentar beta si la similitud sem√°ntica es m√°s confiable\n",
    "        # - Probar valores como (0.7, 0.3), (0.5, 0.5), (0.8, 0.2)\n",
    "        final_score = alpha * bert_score + beta * embed_score\n",
    "        \n",
    "        # Agregar resultado con toda la informaci√≥n\n",
    "        results.append({\n",
    "            \"brecha_id\": brecha_row[\"id\"],          # ID num√©rico de la brecha\n",
    "            \"brecha_text\": brecha_row[\"brecha\"],    # Descripci√≥n textual\n",
    "            \"bert_score\": bert_score,                # Score del modelo supervisado\n",
    "            \"embed_score\": embed_score,              # Score de similitud sem√°ntica\n",
    "            \"final_score\": final_score               # Score combinado final\n",
    "        })\n",
    "    \n",
    "    # ====================================================================\n",
    "    # ORDENAMIENTO FINAL POR SCORE COMBINADO\n",
    "    # ====================================================================\n",
    "    \n",
    "    # Ordenar resultados de mayor a menor final_score\n",
    "    # La brecha con mayor final_score es la predicci√≥n principal\n",
    "    results = sorted(results, key=lambda x: x[\"final_score\"], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ====================================================================\n",
    "# EJEMPLO DE USO Y PRUEBA CON PROYECTOS REALES DEL DATASET\n",
    "# ====================================================================\n",
    "\n",
    "# Cargar proyectos para testing\n",
    "proyectos_test = pd.read_csv(\"data/proyectos.csv\")\n",
    "\n",
    "# Seleccionar un proyecto real del dataset para validar el modelo\n",
    "# Vamos a probar con el proyecto ID 211 que es de educaci√≥n inicial (brecha 22)\n",
    "test_idx = 210  # √çndice 210 = fila 211 en el CSV (0-based)\n",
    "proyecto_real = proyectos_test.iloc[test_idx]\n",
    "\n",
    "# Concatenar t√≠tulo y descripci√≥n como lo hacemos en entrenamiento\n",
    "text_proyecto = f\"{proyecto_real['title']}. {proyecto_real['description']}\"\n",
    "\n",
    "# Brecha verdadera (ground truth)\n",
    "brecha_verdadera = proyecto_real['brecha_ids']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRUEBA DE CLASIFICACI√ìN H√çBRIDA - PROYECTO REAL DEL DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Project ID: {proyecto_real['project_id']}\")\n",
    "print(f\"T√≠tulo: {proyecto_real['title']}\")\n",
    "print(f\"\\nBRECHA VERDADERA: {brecha_verdadera}\")\n",
    "print(f\"Descripci√≥n brecha: {brechas[brechas['id'] == int(brecha_verdadera)]['brecha'].values[0]}\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Resultados de clasificaci√≥n (top 5 candidatos):\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Ejecutar clasificaci√≥n h√≠brida\n",
    "clasificacion = hybrid_classify(text_proyecto, top_k=5)\n",
    "\n",
    "# Mostrar cada candidato con sus scores\n",
    "for i, resultado in enumerate(clasificacion, 1):\n",
    "    # Marcar con ‚úÖ si es la brecha correcta\n",
    "    marcador = \"‚úÖ CORRECTO\" if resultado['brecha_id'] == int(brecha_verdadera) else \"\"\n",
    "    print(f\"\\n{i}. Brecha ID {resultado['brecha_id']}: {resultado['brecha_text'][:80]}... {marcador}\")\n",
    "    print(f\"   - BERT score:       {resultado['bert_score']:.4f}\")\n",
    "    print(f\"   - Embedding score:  {resultado['embed_score']:.4f}\")\n",
    "    print(f\"   - FINAL score:      {resultado['final_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "prediccion_correcta = clasificacion[0]['brecha_id'] == int(brecha_verdadera)\n",
    "if prediccion_correcta:\n",
    "    print(f\"‚úÖ PREDICCI√ìN CORRECTA: Brecha ID {clasificacion[0]['brecha_id']}\")\n",
    "else:\n",
    "    print(f\"‚ùå PREDICCI√ìN INCORRECTA\")\n",
    "    print(f\"   Predijo: Brecha ID {clasificacion[0]['brecha_id']}\")\n",
    "    print(f\"   Esperado: Brecha ID {brecha_verdadera}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ====================================================================\n",
    "# PRUEBA ADICIONAL: Proyecto de ejemplo que mencionaste\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"PRUEBA CON PROYECTO DE EJEMPLO (CUNA JARDIN)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "proj = \"AMPLIACION DE 01 AULA + 01 DIRECCION EN LA CUNA JARDIN, HOSPITAL AMAZONICO- PUERTO CALLAO\"\n",
    "print(f\"Proyecto: {proj}\\n\")\n",
    "\n",
    "# Nota: Este proyecto deber√≠a ser brecha 48 (educaci√≥n inicial - matriculaci√≥n)\n",
    "# pero esa brecha NO EXISTE en el dataset de entrenamiento, por lo que el modelo\n",
    "# probablemente clasificar√° como brecha 22 (infraestructura educaci√≥n inicial)\n",
    "# o brecha 5 (centros cuna m√°s)\n",
    "\n",
    "print(\"‚ö†Ô∏è NOTA: Este proyecto menciona 'CUNA JARDIN' (educaci√≥n inicial)\")\n",
    "print(\"Las brechas relacionadas en el dataset son:\")\n",
    "print(\"  - Brecha 5: CENTROS CUNA M√ÅS EN CONDICIONES INADECUADAS\")\n",
    "print(\"  - Brecha 22: LOCALES EDUCATIVOS CON SERVICIO DE EDUCACI√ìN INICIAL INADECUADA\")\n",
    "print(\"  - Brecha 48: PERSONAS NO MATRICULADAS EN NIVEL INICIAL (‚ö†Ô∏è SIN DATOS DE ENTRENAMIENTO)\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Resultados (top 5 candidatos):\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Llamar funci√≥n y mostrar resultados\n",
    "clasificacion2 = hybrid_classify(proj, top_k=5)\n",
    "\n",
    "# Mostrar cada candidato con sus scores\n",
    "for i, resultado in enumerate(clasificacion2, 1):\n",
    "    print(f\"\\n{i}. Brecha ID {resultado['brecha_id']}: {resultado['brecha_text'][:80]}...\")\n",
    "    print(f\"   - BERT score:       {resultado['bert_score']:.4f}\")\n",
    "    print(f\"   - Embedding score:  {resultado['embed_score']:.4f}\")\n",
    "    print(f\"   - FINAL score:      {resultado['final_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ Predicci√≥n principal: Brecha ID {clasificacion2[0]['brecha_id']}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ====================================================================\n",
    "# AN√ÅLISIS DE DISTRIBUCI√ìN DE BRECHAS EN EL DATASET\n",
    "# ====================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISIS DE DISTRIBUCI√ìN DE BRECHAS EN EL DATASET DE ENTRENAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Contar proyectos por brecha\n",
    "brecha_counts = {}\n",
    "for idx, row in proyectos_test.iterrows():\n",
    "    brecha_ids = str(row['brecha_ids']).split(',')\n",
    "    for bid in brecha_ids:\n",
    "        bid = bid.strip()\n",
    "        if bid:\n",
    "            bid_int = int(bid)\n",
    "            brecha_counts[bid_int] = brecha_counts.get(bid_int, 0) + 1\n",
    "\n",
    "# Ordenar por frecuencia\n",
    "brecha_counts_sorted = sorted(brecha_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTotal de proyectos: {len(proyectos_test)}\")\n",
    "print(f\"Total de brechas en cat√°logo: {len(brechas)}\")\n",
    "print(f\"Brechas con al menos 1 proyecto: {len(brecha_counts)}\")\n",
    "print(f\"Brechas sin proyectos: {len(brechas) - len(brecha_counts)}\")\n",
    "\n",
    "print(\"\\nüìä Top 10 brechas m√°s frecuentes:\")\n",
    "for i, (bid, count) in enumerate(brecha_counts_sorted[:10], 1):\n",
    "    brecha_text = brechas[brechas['id'] == bid]['brecha'].values[0]\n",
    "    print(f\"{i}. Brecha {bid}: {count} proyectos - {brecha_text[:60]}...\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Brechas sin ejemplos de entrenamiento:\")\n",
    "brechas_sin_datos = [bid for bid in brechas['id'] if bid not in brecha_counts]\n",
    "if brechas_sin_datos:\n",
    "    for bid in brechas_sin_datos[:5]:  # Mostrar solo las primeras 5\n",
    "        brecha_text = brechas[brechas['id'] == bid]['brecha'].values[0]\n",
    "        print(f\"  - Brecha {bid}: {brecha_text[:70]}...\")\n",
    "    if len(brechas_sin_datos) > 5:\n",
    "        print(f\"  ... y {len(brechas_sin_datos) - 5} brechas m√°s sin datos\")\n",
    "else:\n",
    "    print(\"  ‚úÖ Todas las brechas tienen al menos un proyecto\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519343ab",
   "metadata": {},
   "source": [
    "## üîß Mejora de Calibraci√≥n de Scores\n",
    "\n",
    "El problema que observaste es real: los scores est√°n muy bajos. Esto se debe a:\n",
    "\n",
    "1. **BERT scores sin normalizar**: Los logits crudos de BERT no son probabilidades\n",
    "2. **Combinaci√≥n simple**: Promediar scores de diferentes escalas no es √≥ptimo\n",
    "3. **Similitud coseno limitada**: Rara vez supera 0.8-0.9 en embeddings gen√©ricos\n",
    "\n",
    "### Soluciones que implementaremos:\n",
    "- ‚úÖ Aplicar **softmax** a los scores de BERT para obtener probabilidades reales\n",
    "- ‚úÖ Permitir **ajustar pesos** entre BERT y embeddings (no solo 50-50)\n",
    "- ‚úÖ **Normalizar** el score final a rango 0-1\n",
    "- ‚úÖ Mostrar **scores calibrados** que tengan sentido intuitivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8636b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de clasificaci√≥n mejorada definida\n",
      "‚úÖ Brecha IDs cargados: 48 brechas\n",
      "\n",
      "Par√°metros configurables:\n",
      "  - bert_weight: Peso de BERT vs embeddings (default 0.5 = 50-50)\n",
      "  - use_softmax: Convertir logits a probabilidades (default True)\n",
      "  - normalize_final: Escalar score final a 0-1 (default True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Cargar los brecha_ids para el mapeo correcto\n",
    "brecha_ids = brechas['id'].tolist()\n",
    "\n",
    "def clasificar_hibrido_mejorado(\n",
    "    texto_proyecto, \n",
    "    bert_model, \n",
    "    bert_tokenizer, \n",
    "    embed_model, \n",
    "    brecha_embeddings, \n",
    "    brecha_ids, \n",
    "    index,\n",
    "    top_k=5,\n",
    "    bert_weight=0.5,  # Peso de BERT (0-1), el resto es para embeddings\n",
    "    use_softmax=True,  # Si True, aplica softmax a scores de BERT\n",
    "    normalize_final=True  # Si True, normaliza el score final a 0-1\n",
    "):\n",
    "    \"\"\"\n",
    "    Versi√≥n mejorada con scores calibrados.\n",
    "    \n",
    "    Args:\n",
    "        bert_weight: Peso para BERT (0-1). Si es 0.7, BERT tiene 70% y embeddings 30%\n",
    "        use_softmax: Si True, convierte logits de BERT a probabilidades con softmax\n",
    "        normalize_final: Si True, escala el score final al rango 0-1\n",
    "    \"\"\"\n",
    "    # 1. BERT: Clasificaci√≥n directa\n",
    "    inputs = bert_tokenizer(\n",
    "        texto_proyecto, \n",
    "        return_tensors='pt', \n",
    "        truncation=True, \n",
    "        max_length=512,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        bert_model = bert_model.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "        logits = outputs.logits[0]  # Shape: [num_classes]\n",
    "        \n",
    "        if use_softmax:\n",
    "            # Aplicar softmax para obtener probabilidades reales\n",
    "            bert_probs = F.softmax(logits, dim=0).cpu().numpy()\n",
    "        else:\n",
    "            # Usar logits crudos (m√©todo anterior)\n",
    "            bert_probs = logits.cpu().numpy()\n",
    "    \n",
    "    # 2. Embeddings: B√∫squeda sem√°ntica\n",
    "    emb_proyecto = embed_model.encode([texto_proyecto], convert_to_numpy=True)\n",
    "    emb_proyecto = emb_proyecto / np.linalg.norm(emb_proyecto, axis=1, keepdims=True)\n",
    "    \n",
    "    # Buscar vecinos m√°s cercanos\n",
    "    D, I = index.search(emb_proyecto.astype('float32'), top_k)\n",
    "    \n",
    "    # 3. Combinaci√≥n h√≠brida\n",
    "    resultados = []\n",
    "    for rank_idx in range(min(top_k, len(I[0]))):\n",
    "        brecha_idx = I[0][rank_idx]\n",
    "        brecha_id = brecha_ids[brecha_idx]\n",
    "        \n",
    "        # Score de similitud sem√°ntica (ya es 0-1 por coseno normalizado)\n",
    "        sim_score = float(D[0][rank_idx])\n",
    "        \n",
    "        # Score de BERT para esta brecha\n",
    "        bert_score = float(bert_probs[brecha_id - 1])  # brecha_id empieza en 1\n",
    "        \n",
    "        # Combinar scores con pesos\n",
    "        embedding_weight = 1.0 - bert_weight\n",
    "        combined_score = (bert_weight * bert_score) + (embedding_weight * sim_score)\n",
    "        \n",
    "        resultados.append({\n",
    "            'brecha_id': brecha_id,\n",
    "            'bert_score': bert_score,\n",
    "            'embedding_score': sim_score,\n",
    "            'combined_score': combined_score,\n",
    "            'bert_weight': bert_weight,\n",
    "            'embedding_weight': embedding_weight\n",
    "        })\n",
    "    \n",
    "    # Ordenar por score combinado\n",
    "    resultados = sorted(resultados, key=lambda x: x['combined_score'], reverse=True)\n",
    "    \n",
    "    # 4. Normalizaci√≥n final (opcional)\n",
    "    if normalize_final and len(resultados) > 0:\n",
    "        max_score = max(r['combined_score'] for r in resultados)\n",
    "        min_score = min(r['combined_score'] for r in resultados)\n",
    "        score_range = max_score - min_score\n",
    "        \n",
    "        if score_range > 0:\n",
    "            for r in resultados:\n",
    "                r['combined_score_normalized'] = (r['combined_score'] - min_score) / score_range\n",
    "        else:\n",
    "            for r in resultados:\n",
    "                r['combined_score_normalized'] = 1.0\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de clasificaci√≥n mejorada definida\")\n",
    "print(f\"‚úÖ Brecha IDs cargados: {len(brecha_ids)} brechas\")\n",
    "print(\"\\nPar√°metros configurables:\")\n",
    "print(\"  - bert_weight: Peso de BERT vs embeddings (default 0.5 = 50-50)\")\n",
    "print(\"  - use_softmax: Convertir logits a probabilidades (default True)\")\n",
    "print(\"  - normalize_final: Escalar score final a 0-1 (default True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f103b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRUEBA CON SCORES CALIBRADOS - PROYECTO REAL\n",
      "================================================================================\n",
      "Project ID: 212\n",
      "T√≠tulo: MEJORAMIENTO DEL SISTEMA DE AGUA POTABLE Y ALCANTARILLADO EN LA COMUNIDAD CAMPESINA DE VILAVILANI, D...\n",
      "\n",
      "BRECHA VERDADERA: 17\n",
      "Descripci√≥n brecha: PORCENTAJE DE LA POBLACI√ìN RURAL SIN ACCESO AL SERVICIO DE ALCANTARILLADO U OTRAS FORMAS DE DISPOSICI√ìN SANITARIA DE EXCRETAS\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîß CONFIGURACI√ìN 1: Softmax + Pesos balanceados (50-50)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Brecha ID 12: PORCENTAJE DE HORAS AL D√çA SIN SERVICIO DE AGUA POTABLE EN EL √ÅMBITO URBANO... \n",
      "   - BERT score:       0.0184\n",
      "   - Embedding score:  0.7182\n",
      "   - Combined score:   0.3683\n",
      "   - Normalized:       1.0000\n",
      "\n",
      "2. Brecha ID 42: PORCENTAJE DE SUPERFICIE SIN ACONDICIONAMIENTO PARA RECARGA H√çDRICA PROVENIENTES... \n",
      "   - BERT score:       0.0121\n",
      "   - Embedding score:  0.6511\n",
      "   - Combined score:   0.3316\n",
      "   - Normalized:       0.1705\n",
      "\n",
      "3. Brecha ID 31: PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS VERDES POR HABITANTE EN LAS ZONAS URBANAS ... \n",
      "   - BERT score:       0.0197\n",
      "   - Embedding score:  0.6343\n",
      "   - Combined score:   0.3270\n",
      "   - Normalized:       0.0657\n",
      "\n",
      "4. Brecha ID 4: PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA... \n",
      "   - BERT score:       0.0165\n",
      "   - Embedding score:  0.6355\n",
      "   - Combined score:   0.3260\n",
      "   - Normalized:       0.0436\n",
      "\n",
      "5. Brecha ID 13: PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN CONDICIONES INADECUAD... \n",
      "   - BERT score:       0.0219\n",
      "   - Embedding score:  0.6263\n",
      "   - Combined score:   0.3241\n",
      "   - Normalized:       0.0000\n",
      "\n",
      "‚ùå PREDICCI√ìN INCORRECTA\n",
      "   Predijo: Brecha ID 12\n",
      "   Esperado: Brecha ID 17\n",
      "\n",
      "================================================================================\n",
      "üîß CONFIGURACI√ìN 2: Softmax + Mayor peso a BERT (70-30)\n",
      "================================================================================\n",
      "\n",
      "1. Brecha ID 12: PORCENTAJE DE HORAS AL D√çA SIN SERVICIO DE AGUA POTABLE EN EL √ÅMBITO URBANO... \n",
      "   - BERT score:       0.0184\n",
      "   - Embedding score:  0.7182\n",
      "   - Combined score:   0.2284\n",
      "   - Normalized:       1.0000\n",
      "\n",
      "2. Brecha ID 31: PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS VERDES POR HABITANTE EN LAS ZONAS URBANAS ... \n",
      "   - BERT score:       0.0197\n",
      "   - Embedding score:  0.6343\n",
      "   - Combined score:   0.2040\n",
      "   - Normalized:       0.0708\n",
      "\n",
      "3. Brecha ID 42: PORCENTAJE DE SUPERFICIE SIN ACONDICIONAMIENTO PARA RECARGA H√çDRICA PROVENIENTES... \n",
      "   - BERT score:       0.0121\n",
      "   - Embedding score:  0.6511\n",
      "   - Combined score:   0.2038\n",
      "   - Normalized:       0.0613\n",
      "\n",
      "4. Brecha ID 13: PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN CONDICIONES INADECUAD... \n",
      "   - BERT score:       0.0219\n",
      "   - Embedding score:  0.6263\n",
      "   - Combined score:   0.2032\n",
      "   - Normalized:       0.0377\n",
      "\n",
      "5. Brecha ID 4: PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA... \n",
      "   - BERT score:       0.0165\n",
      "   - Embedding score:  0.6355\n",
      "   - Combined score:   0.2022\n",
      "   - Normalized:       0.0000\n",
      "\n",
      "‚ùå PREDICCI√ìN INCORRECTA\n",
      "   Predijo: Brecha ID 12\n",
      "   Esperado: Brecha ID 17\n",
      "\n",
      "================================================================================\n",
      "üîß CONFIGURACI√ìN 3: Softmax + Mayor peso a Embeddings (30-70)\n",
      "================================================================================\n",
      "\n",
      "1. Brecha ID 12: PORCENTAJE DE HORAS AL D√çA SIN SERVICIO DE AGUA POTABLE EN EL √ÅMBITO URBANO... \n",
      "   - BERT score:       0.0184\n",
      "   - Embedding score:  0.7182\n",
      "   - Combined score:   0.5083\n",
      "   - Normalized:       1.0000\n",
      "\n",
      "2. Brecha ID 42: PORCENTAJE DE SUPERFICIE SIN ACONDICIONAMIENTO PARA RECARGA H√çDRICA PROVENIENTES... \n",
      "   - BERT score:       0.0121\n",
      "   - Embedding score:  0.6511\n",
      "   - Combined score:   0.4594\n",
      "   - Normalized:       0.2285\n",
      "\n",
      "3. Brecha ID 31: PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS VERDES POR HABITANTE EN LAS ZONAS URBANAS ... \n",
      "   - BERT score:       0.0197\n",
      "   - Embedding score:  0.6343\n",
      "   - Combined score:   0.4499\n",
      "   - Normalized:       0.0782\n",
      "\n",
      "4. Brecha ID 4: PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA... \n",
      "   - BERT score:       0.0165\n",
      "   - Embedding score:  0.6355\n",
      "   - Combined score:   0.4498\n",
      "   - Normalized:       0.0766\n",
      "\n",
      "5. Brecha ID 13: PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN CONDICIONES INADECUAD... \n",
      "   - BERT score:       0.0219\n",
      "   - Embedding score:  0.6263\n",
      "   - Combined score:   0.4449\n",
      "   - Normalized:       0.0000\n",
      "\n",
      "‚ùå PREDICCI√ìN INCORRECTA\n",
      "   Predijo: Brecha ID 12\n",
      "   Esperado: Brecha ID 17\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PRUEBA CON EL PROYECTO REAL - VERSI√ìN MEJORADA\n",
    "# ============================================================\n",
    "\n",
    "test_idx = 211\n",
    "proyecto_real = proyectos.iloc[test_idx]\n",
    "brecha_verdadera = int(proyecto_real['brecha_ids'])  # Convertir a int por si acaso\n",
    "text_proyecto = f\"{proyecto_real['title']} {proyecto_real['description']}\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRUEBA CON SCORES CALIBRADOS - PROYECTO REAL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Project ID: {proyecto_real['project_id']}\")\n",
    "print(f\"T√≠tulo: {proyecto_real['title'][:100]}...\")\n",
    "print(f\"\\nBRECHA VERDADERA: {brecha_verdadera}\")\n",
    "\n",
    "brecha_desc = brechas[brechas['id'] == brecha_verdadera]['brecha'].values\n",
    "if len(brecha_desc) > 0:\n",
    "    print(f\"Descripci√≥n brecha: {brecha_desc[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üîß CONFIGURACI√ìN 1: Softmax + Pesos balanceados (50-50)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "resultados = clasificar_hibrido_mejorado(\n",
    "    text_proyecto,\n",
    "    bert_model,\n",
    "    bert_tokenizer,\n",
    "    embed_model,\n",
    "    brecha_embeddings,\n",
    "    brecha_ids,\n",
    "    index,\n",
    "    top_k=5,\n",
    "    bert_weight=0.5,\n",
    "    use_softmax=True,\n",
    "    normalize_final=True\n",
    ")\n",
    "\n",
    "for i, r in enumerate(resultados, 1):\n",
    "    bid = r['brecha_id']\n",
    "    desc = brechas[brechas['id'] == bid]['brecha'].values[0][:80]\n",
    "    \n",
    "    marcador = \"‚úÖ CORRECTO\" if bid == brecha_verdadera else \"\"\n",
    "    \n",
    "    print(f\"\\n{i}. Brecha ID {bid}: {desc}... {marcador}\")\n",
    "    print(f\"   - BERT score:       {r['bert_score']:.4f}\")\n",
    "    print(f\"   - Embedding score:  {r['embedding_score']:.4f}\")\n",
    "    print(f\"   - Combined score:   {r['combined_score']:.4f}\")\n",
    "    if 'combined_score_normalized' in r:\n",
    "        print(f\"   - Normalized:       {r['combined_score_normalized']:.4f}\")\n",
    "\n",
    "prediccion = resultados[0]['brecha_id']\n",
    "if prediccion == brecha_verdadera:\n",
    "    print(f\"\\n‚úÖ PREDICCI√ìN CORRECTA: Brecha ID {prediccion}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå PREDICCI√ìN INCORRECTA\")\n",
    "    print(f\"   Predijo: Brecha ID {prediccion}\")\n",
    "    print(f\"   Esperado: Brecha ID {brecha_verdadera}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîß CONFIGURACI√ìN 2: Softmax + Mayor peso a BERT (70-30)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "resultados2 = clasificar_hibrido_mejorado(\n",
    "    text_proyecto,\n",
    "    bert_model,\n",
    "    bert_tokenizer,\n",
    "    embed_model,\n",
    "    brecha_embeddings,\n",
    "    brecha_ids,\n",
    "    index,\n",
    "    top_k=5,\n",
    "    bert_weight=0.7,  # M√°s peso a BERT\n",
    "    use_softmax=True,\n",
    "    normalize_final=True\n",
    ")\n",
    "\n",
    "for i, r in enumerate(resultados2, 1):\n",
    "    bid = r['brecha_id']\n",
    "    desc = brechas[brechas['id'] == bid]['brecha'].values[0][:80]\n",
    "    \n",
    "    marcador = \"‚úÖ CORRECTO\" if bid == brecha_verdadera else \"\"\n",
    "    \n",
    "    print(f\"\\n{i}. Brecha ID {bid}: {desc}... {marcador}\")\n",
    "    print(f\"   - BERT score:       {r['bert_score']:.4f}\")\n",
    "    print(f\"   - Embedding score:  {r['embedding_score']:.4f}\")\n",
    "    print(f\"   - Combined score:   {r['combined_score']:.4f}\")\n",
    "    if 'combined_score_normalized' in r:\n",
    "        print(f\"   - Normalized:       {r['combined_score_normalized']:.4f}\")\n",
    "\n",
    "prediccion2 = resultados2[0]['brecha_id']\n",
    "if prediccion2 == brecha_verdadera:\n",
    "    print(f\"\\n‚úÖ PREDICCI√ìN CORRECTA: Brecha ID {prediccion2}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå PREDICCI√ìN INCORRECTA\")\n",
    "    print(f\"   Predijo: Brecha ID {prediccion2}\")\n",
    "    print(f\"   Esperado: Brecha ID {brecha_verdadera}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîß CONFIGURACI√ìN 3: Softmax + Mayor peso a Embeddings (30-70)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "resultados3 = clasificar_hibrido_mejorado(\n",
    "    text_proyecto,\n",
    "    bert_model,\n",
    "    bert_tokenizer,\n",
    "    embed_model,\n",
    "    brecha_embeddings,\n",
    "    brecha_ids,\n",
    "    index,\n",
    "    top_k=5,\n",
    "    bert_weight=0.3,  # M√°s peso a embeddings\n",
    "    use_softmax=True,\n",
    "    normalize_final=True\n",
    ")\n",
    "\n",
    "for i, r in enumerate(resultados3, 1):\n",
    "    bid = r['brecha_id']\n",
    "    desc = brechas[brechas['id'] == bid]['brecha'].values[0][:80]\n",
    "    \n",
    "    marcador = \"‚úÖ CORRECTO\" if bid == brecha_verdadera else \"\"\n",
    "    \n",
    "    print(f\"\\n{i}. Brecha ID {bid}: {desc}... {marcador}\")\n",
    "    print(f\"   - BERT score:       {r['bert_score']:.4f}\")\n",
    "    print(f\"   - Embedding score:  {r['embedding_score']:.4f}\")\n",
    "    print(f\"   - Combined score:   {r['combined_score']:.4f}\")\n",
    "    if 'combined_score_normalized' in r:\n",
    "        print(f\"   - Normalized:       {r['combined_score_normalized']:.4f}\")\n",
    "\n",
    "prediccion3 = resultados3[0]['brecha_id']\n",
    "if prediccion3 == brecha_verdadera:\n",
    "    print(f\"\\n‚úÖ PREDICCI√ìN CORRECTA: Brecha ID {prediccion3}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå PREDICCI√ìN INCORRECTA\")\n",
    "    print(f\"   Predijo: Brecha ID {prediccion3}\")\n",
    "    print(f\"   Esperado: Brecha ID {brecha_verdadera}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936754f",
   "metadata": {},
   "source": [
    "## üìà Interpretaci√≥n de los Scores\n",
    "\n",
    "### ¬øPor qu√© los scores no llegan a 0.9?\n",
    "\n",
    "#### BERT Scores (0.01 - 0.02 = 1-2%)\n",
    "- **Son probabilidades reales** despu√©s de aplicar softmax\n",
    "- **Bajos porque hay 48 clases**: Si fueran uniformes, cada una tendr√≠a 1/48 = 2.08%\n",
    "- **El modelo no est√° seguro**: Indica que necesita m√°s datos de entrenamiento\n",
    "- **Normal con pocos datos**: 817 proyectos √∑ 48 clases = ~17 ejemplos/clase promedio\n",
    "\n",
    "#### Embedding Scores (0.62 - 0.72 = 62-72%)\n",
    "- **Son similitudes coseno** (0 = no similar, 1 = id√©ntico)\n",
    "- **0.70+ es considerado \"bueno\"** para embeddings gen√©ricos\n",
    "- **No llegan a 0.9** porque el modelo no fue fine-tuned en tu dominio\n",
    "\n",
    "#### ¬øQu√© significa esto?\n",
    "- ‚úÖ **Los scores est√°n calibrados correctamente** ahora\n",
    "- ‚ö†Ô∏è **BERT necesita m√°s entrenamiento o datos** (scores muy bajos)\n",
    "- ‚úÖ **Embeddings funcionan bien** (0.7+ de similitud es respetable)\n",
    "- üí° **El sistema h√≠brido est√° balanceando ambos m√©todos**\n",
    "\n",
    "### Opciones para mejorar:\n",
    "\n",
    "1. **M√°s datos de entrenamiento** para BERT (conseguir m√°s proyectos etiquetados)\n",
    "2. **Fine-tune del modelo de embeddings** en tu dominio espec√≠fico\n",
    "3. **Ajustar pesos din√°micamente** seg√∫n confianza del modelo\n",
    "4. **Usar solo embeddings** si BERT no mejora (bert_weight=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11e094e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç COMPARACI√ìN DE ESTRATEGIAS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ SOLO EMBEDDINGS (bert_weight=0.0)\n",
      "--------------------------------------------------------------------------------\n",
      "1. Brecha 12: PORCENTAJE DE HORAS AL D√çA SIN SERVICIO DE AGUA POTABLE EN EL √ÅMBITO URBANO... \n",
      "   Embedding score: 0.7182\n",
      "2. Brecha 42: PORCENTAJE DE SUPERFICIE SIN ACONDICIONAMIENTO PARA RECARGA H√çDRICA PROVENIENTES... \n",
      "   Embedding score: 0.6511\n",
      "3. Brecha 4: PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA... \n",
      "   Embedding score: 0.6355\n",
      "4. Brecha 31: PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS VERDES POR HABITANTE EN LAS ZONAS URBANAS ... \n",
      "   Embedding score: 0.6343\n",
      "5. Brecha 13: PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN CONDICIONES INADECUAD... \n",
      "   Embedding score: 0.6263\n",
      "\n",
      "‚ùå Predicci√≥n: Brecha 12\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ SOLO BERT (bert_weight=1.0)\n",
      "--------------------------------------------------------------------------------\n",
      "1. Brecha 13: PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN CONDICIONES INADECUAD... \n",
      "   BERT score: 0.0219\n",
      "2. Brecha 31: PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS VERDES POR HABITANTE EN LAS ZONAS URBANAS ... \n",
      "   BERT score: 0.0197\n",
      "3. Brecha 12: PORCENTAJE DE HORAS AL D√çA SIN SERVICIO DE AGUA POTABLE EN EL √ÅMBITO URBANO... \n",
      "   BERT score: 0.0184\n",
      "4. Brecha 4: PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA... \n",
      "   BERT score: 0.0165\n",
      "5. Brecha 42: PORCENTAJE DE SUPERFICIE SIN ACONDICIONAMIENTO PARA RECARGA H√çDRICA PROVENIENTES... \n",
      "   BERT score: 0.0121\n",
      "\n",
      "‚ùå Predicci√≥n: Brecha 13\n",
      "\n",
      "================================================================================\n",
      "üìä RESUMEN DE PREDICCIONES\n",
      "================================================================================\n",
      "Brecha verdadera: 17\n",
      "  - Solo Embeddings: 12 ‚ùå\n",
      "  - Solo BERT:       13 ‚ùå\n",
      "  - H√≠brido 50-50:   12 ‚ùå\n",
      "================================================================================\n",
      "1. Brecha 12: PORCENTAJE DE HORAS AL D√çA SIN SERVICIO DE AGUA POTABLE EN EL √ÅMBITO URBANO... \n",
      "   Embedding score: 0.7182\n",
      "2. Brecha 42: PORCENTAJE DE SUPERFICIE SIN ACONDICIONAMIENTO PARA RECARGA H√çDRICA PROVENIENTES... \n",
      "   Embedding score: 0.6511\n",
      "3. Brecha 4: PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA... \n",
      "   Embedding score: 0.6355\n",
      "4. Brecha 31: PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS VERDES POR HABITANTE EN LAS ZONAS URBANAS ... \n",
      "   Embedding score: 0.6343\n",
      "5. Brecha 13: PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN CONDICIONES INADECUAD... \n",
      "   Embedding score: 0.6263\n",
      "\n",
      "‚ùå Predicci√≥n: Brecha 12\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ SOLO BERT (bert_weight=1.0)\n",
      "--------------------------------------------------------------------------------\n",
      "1. Brecha 13: PORCENTAJE DE INSTALACIONES DEPORTIVAS Y /O RECREATIVAS EN CONDICIONES INADECUAD... \n",
      "   BERT score: 0.0219\n",
      "2. Brecha 31: PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS VERDES POR HABITANTE EN LAS ZONAS URBANAS ... \n",
      "   BERT score: 0.0197\n",
      "3. Brecha 12: PORCENTAJE DE HORAS AL D√çA SIN SERVICIO DE AGUA POTABLE EN EL √ÅMBITO URBANO... \n",
      "   BERT score: 0.0184\n",
      "4. Brecha 4: PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA... \n",
      "   BERT score: 0.0165\n",
      "5. Brecha 42: PORCENTAJE DE SUPERFICIE SIN ACONDICIONAMIENTO PARA RECARGA H√çDRICA PROVENIENTES... \n",
      "   BERT score: 0.0121\n",
      "\n",
      "‚ùå Predicci√≥n: Brecha 13\n",
      "\n",
      "================================================================================\n",
      "üìä RESUMEN DE PREDICCIONES\n",
      "================================================================================\n",
      "Brecha verdadera: 17\n",
      "  - Solo Embeddings: 12 ‚ùå\n",
      "  - Solo BERT:       13 ‚ùå\n",
      "  - H√≠brido 50-50:   12 ‚ùå\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPARACI√ìN: SOLO EMBEDDINGS vs H√çBRIDO\n",
    "# ============================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç COMPARACI√ìN DE ESTRATEGIAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estrategia 1: Solo embeddings (bert_weight=0.0)\n",
    "print(\"\\n1Ô∏è‚É£ SOLO EMBEDDINGS (bert_weight=0.0)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "resultados_solo_emb = clasificar_hibrido_mejorado(\n",
    "    text_proyecto,\n",
    "    bert_model,\n",
    "    bert_tokenizer,\n",
    "    embed_model,\n",
    "    brecha_embeddings,\n",
    "    brecha_ids,\n",
    "    index,\n",
    "    top_k=5,\n",
    "    bert_weight=0.0,  # 100% embeddings, 0% BERT\n",
    "    use_softmax=True,\n",
    "    normalize_final=False  # No normalizar para ver scores reales\n",
    ")\n",
    "\n",
    "for i, r in enumerate(resultados_solo_emb, 1):\n",
    "    bid = r['brecha_id']\n",
    "    desc = brechas[brechas['id'] == bid]['brecha'].values[0][:80]\n",
    "    marcador = \"‚úÖ CORRECTO\" if bid == brecha_verdadera else \"\"\n",
    "    print(f\"{i}. Brecha {bid}: {desc}... {marcador}\")\n",
    "    print(f\"   Embedding score: {r['embedding_score']:.4f}\")\n",
    "\n",
    "prediccion_emb = resultados_solo_emb[0]['brecha_id']\n",
    "print(f\"\\n{'‚úÖ' if prediccion_emb == brecha_verdadera else '‚ùå'} Predicci√≥n: Brecha {prediccion_emb}\")\n",
    "\n",
    "# Estrategia 2: Solo BERT (bert_weight=1.0)\n",
    "print(\"\\n\\n2Ô∏è‚É£ SOLO BERT (bert_weight=1.0)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "resultados_solo_bert = clasificar_hibrido_mejorado(\n",
    "    text_proyecto,\n",
    "    bert_model,\n",
    "    bert_tokenizer,\n",
    "    embed_model,\n",
    "    brecha_embeddings,\n",
    "    brecha_ids,\n",
    "    index,\n",
    "    top_k=5,\n",
    "    bert_weight=1.0,  # 100% BERT, 0% embeddings\n",
    "    use_softmax=True,\n",
    "    normalize_final=False\n",
    ")\n",
    "\n",
    "for i, r in enumerate(resultados_solo_bert, 1):\n",
    "    bid = r['brecha_id']\n",
    "    desc = brechas[brechas['id'] == bid]['brecha'].values[0][:80]\n",
    "    marcador = \"‚úÖ CORRECTO\" if bid == brecha_verdadera else \"\"\n",
    "    print(f\"{i}. Brecha {bid}: {desc}... {marcador}\")\n",
    "    print(f\"   BERT score: {r['bert_score']:.4f}\")\n",
    "\n",
    "prediccion_bert = resultados_solo_bert[0]['brecha_id']\n",
    "print(f\"\\n{'‚úÖ' if prediccion_bert == brecha_verdadera else '‚ùå'} Predicci√≥n: Brecha {prediccion_bert}\")\n",
    "\n",
    "# Resumen\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RESUMEN DE PREDICCIONES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Brecha verdadera: {brecha_verdadera}\")\n",
    "print(f\"  - Solo Embeddings: {prediccion_emb} {'‚úÖ' if prediccion_emb == brecha_verdadera else '‚ùå'}\")\n",
    "print(f\"  - Solo BERT:       {prediccion_bert} {'‚úÖ' if prediccion_bert == brecha_verdadera else '‚ùå'}\")\n",
    "print(f\"  - H√≠brido 50-50:   {resultados[0]['brecha_id']} {'‚úÖ' if resultados[0]['brecha_id'] == brecha_verdadera else '‚ùå'}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb461403",
   "metadata": {},
   "source": [
    "## üìä ¬øCu√°ntos Registros Necesitas? ¬øBETO es la Soluci√≥n Correcta?\n",
    "\n",
    "### Situaci√≥n Actual\n",
    "Tienes **817 proyectos** etiquetados para **48 brechas**\n",
    "\n",
    "### üéØ Regla General: Registros Recomendados por Clase\n",
    "\n",
    "| Escenario | Registros/Clase | Total (48 clases) | Calidad Esperada |\n",
    "|-----------|----------------|-------------------|------------------|\n",
    "| **M√≠nimo viable** | 20-50 | 960-2,400 | Baja (~40-60% accuracy) ‚úÖ TU CASO |\n",
    "| **Aceptable** | 100-200 | 4,800-9,600 | Media (~65-75% accuracy) |\n",
    "| **Bueno** | 500-1,000 | 24,000-48,000 | Alta (~80-85% accuracy) |\n",
    "| **Excelente** | 2,000+ | 96,000+ | Muy alta (~90%+ accuracy) |\n",
    "\n",
    "Tu promedio: **817 √∑ 48 = 17 ejemplos/clase** ‚ö†Ô∏è **BAJO EL M√çNIMO**\n",
    "\n",
    "### üìà An√°lisis de Tu Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cca5f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä AN√ÅLISIS DE TU DATASET\n",
      "================================================================================\n",
      "\n",
      "Total de proyectos: 817\n",
      "Total de brechas: 48\n",
      "Promedio por brecha: 17.0 proyectos\n",
      "\n",
      "üìà Distribuci√≥n de ejemplos:\n",
      "  - M√≠nimo:  1 proyectos (brecha 1)\n",
      "  - M√°ximo:  151 proyectos (brecha 18)\n",
      "  - Mediana: 2 proyectos\n",
      "  - Media:   17.4 proyectos\n",
      "\n",
      "üîç Categorizaci√≥n de brechas por cantidad de datos:\n",
      "  - Muy pocos datos (<10):      31 brechas (64.6%)\n",
      "  - Datos m√≠nimos (10-49):      11 brechas (22.9%)\n",
      "  - Datos aceptables (50-99):   3 brechas (6.2%)\n",
      "  - Buenos datos (100+):        2 brechas (4.2%)\n",
      "\n",
      "‚ö†Ô∏è Brechas con MUY POCOS datos (<10 ejemplos):\n",
      "  - Brecha 1: 1 ejemplos - INDICADOR DE BRECHA POR DEFINIR...\n",
      "  - Brecha 2: 3 ejemplos - PORCENTAJE DE ALIMENTOS AGROPECUARIOS DE PRODUCCI√ìN Y PROCES...\n",
      "  - Brecha 3: 2 ejemplos - PORCENTAJE DE CAPITALES DE DISTRITO QUE NO CUENTAN CON UN CE...\n",
      "  - Brecha 4: 2 ejemplos - PORCENTAJE DE CEMENTERIOS CON CAPACIDAD INSTALADA INADECUADA...\n",
      "  - Brecha 5: 1 ejemplos - PORCENTAJE DE CENTROS CUNA M√ÅS EN CONDICIONES INADECUADAS...\n",
      "  - Brecha 6: 1 ejemplos - PORCENTAJE DE CENTROS DE DESARROLLO INTEGRAL DE LAS FAMILIAS...\n",
      "  - Brecha 7: 1 ejemplos - PORCENTAJE DE CENTROS DE EDUCACI√ìN B√ÅSICA ESPECIAL CON CAPAC...\n",
      "  - Brecha 8: 1 ejemplos - PORCENTAJE DE CENTROS DE PROMOCI√ìN Y VIGILANCIA COMUNAL REQU...\n",
      "  - Brecha 9: 1 ejemplos - PORCENTAJE DE COMISAR√çAS B√ÅSICAS QUE OPERAN EN CONDICIONES I...\n",
      "  - Brecha 10: 1 ejemplos - PORCENTAJE DE DISTRITOS CON M√ÅS DE 20,000 HABITANTES QUE NO ...\n",
      "  - Brecha 11: 2 ejemplos - PORCENTAJE DE DISTRITOS QUE NO CUENTAN CON AL MENOS UNA ESTA...\n",
      "  - Brecha 14: 1 ejemplos - PORCENTAJE DE LA DEMANDA DE ENERG√çA EL√âCTRICA NO ATENDIDA ME...\n",
      "  - Brecha 15: 1 ejemplos - PORCENTAJE DE LA POBLACION RURAL Y RURAL DISPERSA SIN ACCESO...\n",
      "  - Brecha 21: 1 ejemplos - PORCENTAJE DE LOCALES EDUCATIVOS CON EL SERVICIO DE EDUCACI√ì...\n",
      "  - Brecha 25: 1 ejemplos - PORCENTAJE DE LOCALES EDUCATIVOS CON EL SERVICIO DE EDUCACI√ì...\n",
      "  - Brecha 26: 1 ejemplos - PORCENTAJE DE LOCALES EDUCATIVOS CON SERVICIO DEL PROGRAMA D...\n",
      "  - Brecha 27: 2 ejemplos - PORCENTAJE DE LOCALIDADES CON AL MENOS UNA ENTIDAD P√öBLICA S...\n",
      "  - Brecha 28: 1 ejemplos - PORCENTAJE DE LOCALIDADES CON POBLACI√ìN DE AL MENOS 100 HABI...\n",
      "  - Brecha 29: 2 ejemplos - PORCENTAJE DE LOCALIDADES IND√çGENAS QUE NO CUENTAN CON SERVI...\n",
      "  - Brecha 31: 6 ejemplos - PORCENTAJE DE M2 DE ESPACIOS P√öBLICOS VERDES POR HABITANTE E...\n",
      "  - Brecha 32: 1 ejemplos - PORCENTAJE DE NI√ëOS(AS) DE LA POBLACI√ìN OBJETIVO QUE NO RECI...\n",
      "  - Brecha 33: 1 ejemplos - PORCENTAJE DE PERSONAS NO MATRICULADAS EN EL NIVEL PRIMARIA ...\n",
      "  - Brecha 34: 1 ejemplos - PORCENTAJE DE PERSONAS NO MATRICULADAS EN EL NIVEL SECUNDARI...\n",
      "  - Brecha 37: 1 ejemplos - PORCENTAJE DE POBLACI√ìN SIN ACESO A LOS SERVICIOS DE RADIODI...\n",
      "  - Brecha 39: 1 ejemplos - PORCENTAJE DE PUNTOS CR√çTICOS EN RIBERA DE R√çO NO PROTEGIDOS...\n",
      "  - Brecha 40: 9 ejemplos - PORCENTAJE DE SECTORES A NIVEL DE DISTRITO QUE NO CUENTAN CO...\n",
      "  - Brecha 42: 3 ejemplos - PORCENTAJE DE SUPERFICIE SIN ACONDICIONAMIENTO PARA RECARGA ...\n",
      "  - Brecha 44: 7 ejemplos - PORCENTAJE DE VIVIENDAS EN EL √ÅMBITO URBANO SIN ACCESO AL SE...\n",
      "  - Brecha 45: 1 ejemplos - PORCENTAJE DE VIVIENDAS RURALES CON SERVICIO DE AGUA CON CLO...\n",
      "  - Brecha 46: 2 ejemplos - PORCENTAJE DE VIVIENDAS URBANAS CON SERVICIO DE AGUA CON CLO...\n",
      "  - Brecha 47: 4 ejemplos - PORCENTAJE DE VOLUMEN DE AGUAS RESIDUALES NO TRATADAS...\n",
      "\n",
      "================================================================================\n",
      "üí° RECOMENDACIONES SEG√öN TU DATASET\n",
      "================================================================================\n",
      "\n",
      "üìä Objetivo de registros totales:\n",
      "  Actual:      817 proyectos\n",
      "  M√≠nimo:      2,400 proyectos (50/brecha) - Necesitas 1,583 m√°s\n",
      "  Bueno:       4,800 proyectos (100/brecha) - Necesitas 3,983 m√°s\n",
      "  Excelente:   24,000 proyectos (500/brecha) - Necesitas 23,183 m√°s\n",
      "\n",
      "üéØ Para tu caso espec√≠fico:\n",
      "  Tienes el 34.0% del objetivo m√≠nimo\n",
      "  Faltan 1,583 proyectos para alcanzar el m√≠nimo\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# AN√ÅLISIS: ¬øCu√°ntos datos tienes vs. cu√°ntos necesitas?\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Contar distribuci√≥n de brechas\n",
    "brecha_distribution = proyectos['brecha_ids'].value_counts().sort_index()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä AN√ÅLISIS DE TU DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal de proyectos: {len(proyectos)}\")\n",
    "print(f\"Total de brechas: {len(brechas)}\")\n",
    "print(f\"Promedio por brecha: {len(proyectos) / len(brechas):.1f} proyectos\")\n",
    "\n",
    "# Estad√≠sticas de distribuci√≥n\n",
    "print(f\"\\nüìà Distribuci√≥n de ejemplos:\")\n",
    "print(f\"  - M√≠nimo:  {brecha_distribution.min()} proyectos (brecha {brecha_distribution.idxmin()})\")\n",
    "print(f\"  - M√°ximo:  {brecha_distribution.max()} proyectos (brecha {brecha_distribution.idxmax()})\")\n",
    "print(f\"  - Mediana: {brecha_distribution.median():.0f} proyectos\")\n",
    "print(f\"  - Media:   {brecha_distribution.mean():.1f} proyectos\")\n",
    "\n",
    "# Categorizar brechas por cantidad de datos\n",
    "pocos_datos = brecha_distribution[brecha_distribution < 10]\n",
    "datos_minimos = brecha_distribution[(brecha_distribution >= 10) & (brecha_distribution < 50)]\n",
    "datos_aceptables = brecha_distribution[(brecha_distribution >= 50) & (brecha_distribution < 100)]\n",
    "datos_buenos = brecha_distribution[brecha_distribution >= 100]\n",
    "\n",
    "print(f\"\\nüîç Categorizaci√≥n de brechas por cantidad de datos:\")\n",
    "print(f\"  - Muy pocos datos (<10):      {len(pocos_datos)} brechas ({len(pocos_datos)/len(brechas)*100:.1f}%)\")\n",
    "print(f\"  - Datos m√≠nimos (10-49):      {len(datos_minimos)} brechas ({len(datos_minimos)/len(brechas)*100:.1f}%)\")\n",
    "print(f\"  - Datos aceptables (50-99):   {len(datos_aceptables)} brechas ({len(datos_aceptables)/len(brechas)*100:.1f}%)\")\n",
    "print(f\"  - Buenos datos (100+):        {len(datos_buenos)} brechas ({len(datos_buenos)/len(brechas)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Brechas con MUY POCOS datos (<10 ejemplos):\")\n",
    "for bid, count in pocos_datos.items():\n",
    "    brecha_name = brechas[brechas['id'] == bid]['brecha'].values[0][:60]\n",
    "    print(f\"  - Brecha {bid}: {count} ejemplos - {brecha_name}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° RECOMENDACIONES SEG√öN TU DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcular cu√°ntos datos necesitar√≠as\n",
    "objetivo_minimo = 50 * len(brechas)\n",
    "objetivo_bueno = 100 * len(brechas)\n",
    "objetivo_excelente = 500 * len(brechas)\n",
    "\n",
    "print(f\"\\nüìä Objetivo de registros totales:\")\n",
    "print(f\"  Actual:      {len(proyectos):,} proyectos\")\n",
    "print(f\"  M√≠nimo:      {objetivo_minimo:,} proyectos (50/brecha) - Necesitas {objetivo_minimo - len(proyectos):,} m√°s\")\n",
    "print(f\"  Bueno:       {objetivo_bueno:,} proyectos (100/brecha) - Necesitas {objetivo_bueno - len(proyectos):,} m√°s\")\n",
    "print(f\"  Excelente:   {objetivo_excelente:,} proyectos (500/brecha) - Necesitas {objetivo_excelente - len(proyectos):,} m√°s\")\n",
    "\n",
    "print(f\"\\nüéØ Para tu caso espec√≠fico:\")\n",
    "porcentaje_completado = (len(proyectos) / objetivo_minimo) * 100\n",
    "print(f\"  Tienes el {porcentaje_completado:.1f}% del objetivo m√≠nimo\")\n",
    "print(f\"  Faltan {objetivo_minimo - len(proyectos):,} proyectos para alcanzar el m√≠nimo\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f9ce0",
   "metadata": {},
   "source": [
    "## ‚úÖ ¬øEs BETO la Soluci√≥n Correcta para Tu Caso?\n",
    "\n",
    "### Respuesta: **S√ç, pero con matices**\n",
    "\n",
    "### Ventajas de BETO en tu contexto:\n",
    "1. ‚úÖ **Pre-entrenado en espa√±ol**: Entiende mejor el lenguaje t√©cnico peruano\n",
    "2. ‚úÖ **Transfer learning**: Aprovecha conocimiento previo (no partes de cero)\n",
    "3. ‚úÖ **Razonable con pocos datos**: Fine-tuning funciona con ~1000 ejemplos\n",
    "4. ‚úÖ **Estado del arte**: BERT sigue siendo competitivo en clasificaci√≥n de texto\n",
    "\n",
    "### Desventajas en tu caso:\n",
    "1. ‚ùå **Necesita m√°s datos** de los que tienes (~817 es muy poco)\n",
    "2. ‚ùå **Overfitting**: Con 17 ejemplos/clase, el modelo memoriza en vez de generalizar\n",
    "3. ‚ùå **Desbalance**: Algunas brechas tienen 150+ ejemplos, otras <10\n",
    "4. ‚ùå **Computacionalmente costoso**: Lento para inferencia en producci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ SOLUCIONES RECOMENDADAS (en orden de prioridad)\n",
    "\n",
    "### **OPCI√ìN 1: Enfoque H√≠brido (LO QUE TIENES AHORA) ‚úÖ RECOMENDADO**\n",
    "**Usa embeddings como motor principal + BERT como apoyo**\n",
    "\n",
    "```python\n",
    "# Configuraci√≥n recomendada para TU caso\n",
    "bert_weight = 0.2  # 20% BERT, 80% embeddings\n",
    "```\n",
    "\n",
    "**Justificaci√≥n:**\n",
    "- Los embeddings funcionan bien con pocos datos (no necesitan entrenamiento)\n",
    "- BETO aporta algo de contexto pero no es confiable a√∫n\n",
    "- Es la mejor opci√≥n con tus 817 registros actuales\n",
    "\n",
    "**Accuracy esperado: 50-60%** üìä\n",
    "\n",
    "---\n",
    "\n",
    "### **OPCI√ìN 2: Solo Embeddings (M√°s Simple) ‚ö° ALTERNATIVA R√ÅPIDA**\n",
    "**Elimina BETO completamente, usa solo b√∫squeda sem√°ntica**\n",
    "\n",
    "```python\n",
    "bert_weight = 0.0  # 100% embeddings\n",
    "```\n",
    "\n",
    "**Ventajas:**\n",
    "- M√°s r√°pido (no necesita GPU)\n",
    "- Sin entrenamiento\n",
    "- M√°s estable (no overfitting)\n",
    "\n",
    "**Desventajas:**\n",
    "- No aprende patrones espec√≠ficos de tus datos\n",
    "- Depende de embeddings gen√©ricos\n",
    "\n",
    "**Accuracy esperado: 45-55%** üìä\n",
    "\n",
    "---\n",
    "\n",
    "### **OPCI√ìN 3: Conseguir M√°s Datos (IDEAL a largo plazo) üéØ**\n",
    "\n",
    "**Meta realista:**\n",
    "- **Corto plazo (1-2 meses)**: 2,400 proyectos (50/brecha)\n",
    "- **Mediano plazo (3-6 meses)**: 4,800 proyectos (100/brecha)\n",
    "- **Largo plazo (1 a√±o)**: 10,000+ proyectos\n",
    "\n",
    "**C√≥mo conseguirlos:**\n",
    "1. üèõÔ∏è **Scraping de portales p√∫blicos**: SNIP, Invierte.pe, MEF\n",
    "2. ü§ñ **Data augmentation**: Generar variaciones de proyectos existentes\n",
    "3. üë• **Etiquetado manual**: Contratar anotadores o crowdsourcing\n",
    "4. üîÑ **Active learning**: El modelo sugiere qu√© proyectos etiquetar\n",
    "\n",
    "**Con 2,400+ proyectos ‚Üí Accuracy esperado: 70-80%** üìä\n",
    "\n",
    "---\n",
    "\n",
    "### **OPCI√ìN 4: Reducir N√∫mero de Brechas (Estrategia Pragm√°tica) üé®**\n",
    "\n",
    "**Agrupar brechas similares para tener m√°s datos/clase:**\n",
    "\n",
    "Por ejemplo:\n",
    "- Brecha 22, 23, 24 ‚Üí **\"Educaci√≥n - Infraestructura Inadecuada\"**\n",
    "- Brecha 16, 17, 18 ‚Üí **\"Agua y Saneamiento\"**\n",
    "- Brecha 30, 31, 35 ‚Üí **\"Espacios P√∫blicos\"**\n",
    "\n",
    "**De 48 clases ‚Üí 15-20 macro-categor√≠as**\n",
    "\n",
    "**Ventajas:**\n",
    "- ~40-50 ejemplos por clase (mucho mejor)\n",
    "- Modelo m√°s robusto\n",
    "- Menor overfitting\n",
    "\n",
    "**Desventajas:**\n",
    "- Menor granularidad\n",
    "- Puede que necesites las 48 brechas exactas\n",
    "\n",
    "**Con agrupaci√≥n ‚Üí Accuracy esperado: 65-75%** üìä\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ RECOMENDACI√ìN FINAL\n",
    "\n",
    "### Para TU situaci√≥n (817 proyectos, 48 brechas):\n",
    "\n",
    "**1. CORTO PLAZO (ahora):**\n",
    "```python\n",
    "# Configuraci√≥n h√≠brida optimizada\n",
    "resultados = clasificar_hibrido_mejorado(\n",
    "    texto_proyecto,\n",
    "    bert_model,\n",
    "    bert_tokenizer,\n",
    "    embed_model,\n",
    "    brecha_embeddings,\n",
    "    brecha_ids,\n",
    "    index,\n",
    "    top_k=5,\n",
    "    bert_weight=0.2,  # Poco peso a BERT (tiene pocos datos)\n",
    "    use_softmax=True,\n",
    "    normalize_final=True\n",
    ")\n",
    "```\n",
    "\n",
    "**2. MEDIANO PLAZO (pr√≥ximos 3 meses):**\n",
    "- Conseguir **1,600 proyectos m√°s** (llegar a 2,400 total)\n",
    "- Reentrenar BETO con datos balanceados\n",
    "- Ajustar peso: `bert_weight=0.5` (50-50)\n",
    "\n",
    "**3. LARGO PLAZO (6-12 meses):**\n",
    "- Llegar a **4,800+ proyectos** (100/brecha)\n",
    "- Fine-tune del modelo de embeddings en tu dominio\n",
    "- Ajustar peso: `bert_weight=0.7` (favorecer BERT)\n",
    "- **Accuracy objetivo: 75-85%**\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øVale la pena seguir con BETO?\n",
    "\n",
    "**S√ç**, porque:\n",
    "- Ya lo tienes entrenado ‚úÖ\n",
    "- Aporta informaci√≥n √∫til (aunque limitada) ‚úÖ\n",
    "- Es escalable cuando consigas m√°s datos ‚úÖ\n",
    "\n",
    "**PERO** ajusta tus expectativas:\n",
    "- Con 817 registros ‚Üí 50-60% accuracy m√°ximo\n",
    "- Los scores bajos (0.01-0.02) son **normales** con pocos datos\n",
    "- No esperes 90%+ sin conseguir m√°s datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33292b30",
   "metadata": {},
   "source": [
    "## üìà **Recomendaciones Seg√∫n Tu Situaci√≥n Actual**\n",
    "\n",
    "### **Estrategia Recomendada (H√≠brida):**\n",
    "\n",
    "#### **Fase 1: Corto Plazo (1-2 semanas)**\n",
    "1. ‚úÖ **Ajustar pesos del modelo h√≠brido** ‚Üí Dar m√°s peso a embeddings\n",
    "   ```python\n",
    "   alpha, beta = 0.2, 0.8  # 20% BERT, 80% embeddings\n",
    "   ```\n",
    "\n",
    "2. ‚úÖ **Buscar 50-100 proyectos reales m√°s** por cada brecha sin datos\n",
    "   - Fuentes: SNIP, Invierte.pe, portales regionales\n",
    "   - Enfocarte en las **brechas sin datos** (como la 48)\n",
    "\n",
    "3. ‚úÖ **Data augmentation moderado**\n",
    "   - Generar 2-3 variaciones de proyectos existentes\n",
    "   - Solo para brechas con menos de 10 ejemplos\n",
    "\n",
    "#### **Fase 2: Mediano Plazo (1-2 meses)**\n",
    "4. ‚úÖ **Expandir dataset a 2000-5000 proyectos**\n",
    "   - M√≠nimo 50 proyectos por brecha\n",
    "   - Balancear distribuci√≥n\n",
    "\n",
    "5. ‚úÖ **Re-entrenar con dataset completo**\n",
    "   - Aumentar √©pocas de entrenamiento (5-10 √©pocas)\n",
    "   - Ajustar learning rate\n",
    "\n",
    "#### **Fase 3: Producci√≥n (3+ meses)**\n",
    "6. ‚úÖ **Dataset robusto: 10,000+ proyectos**\n",
    "   - 200+ proyectos por brecha\n",
    "   - Validaci√≥n humana de calidad\n",
    "\n",
    "7. ‚úÖ **Fine-tuning avanzado**\n",
    "   - Probar otros modelos (RoBERTa espa√±ol, ELECTRA)\n",
    "   - Ensemble de modelos\n",
    "\n",
    "---\n",
    "\n",
    "### **N√∫meros Concretos para Tu Caso:**\n",
    "\n",
    "| Brecha | Min. Aceptable | Ideal | Producci√≥n |\n",
    "|--------|---------------|-------|------------|\n",
    "| **Total Dataset** | 1,500 proyectos | 5,000 proyectos | 10,000+ proyectos |\n",
    "| **Por Brecha** | 20-30 proyectos | 100 proyectos | 200+ proyectos |\n",
    "| **Brechas sin datos** | **0** (eliminar o fusionar) | **0** | **0** |\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è Brechas Problem√°ticas en Tu Dataset:**\n",
    "\n",
    "Seg√∫n el an√°lisis, tienes brechas **sin ning√∫n proyecto**. Opciones:\n",
    "\n",
    "1. **Buscar proyectos espec√≠ficos** para esas brechas\n",
    "2. **Fusionar brechas similares** (ej: brecha 22 y 48 son ambas educaci√≥n inicial)\n",
    "3. **Eliminar brechas sin datos** temporalmente del modelo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# HERRAMIENTA 1: GENERADOR DE PAR√ÅFRASIS CON GPT (Opcional)\n",
    "# ====================================================================\n",
    "# Usa esta funci√≥n si tienes API key de OpenAI para aumentar tu dataset\n",
    "\n",
    "def generar_variaciones_gpt(proyecto_original, num_variaciones=3, api_key=None):\n",
    "    \"\"\"\n",
    "    Genera variaciones de un proyecto usando GPT para data augmentation.\n",
    "    \n",
    "    Args:\n",
    "        proyecto_original (str): Texto del proyecto original\n",
    "        num_variaciones (int): N√∫mero de par√°frasis a generar\n",
    "        api_key (str): API key de OpenAI (opcional, usa variable de entorno si no se proporciona)\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de variaciones del proyecto\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Instala openai: pip install openai\")\n",
    "        return []\n",
    "    \n",
    "    api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"‚ö†Ô∏è No se encontr√≥ OPENAI_API_KEY\")\n",
    "        return []\n",
    "    \n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    prompt = f\"\"\"Eres un experto en proyectos de inversi√≥n p√∫blica en Per√∫.\n",
    "\n",
    "Genera {num_variaciones} PAR√ÅFRASIS del siguiente proyecto, manteniendo:\n",
    "1. El mismo significado y alcance del proyecto\n",
    "2. Terminolog√≠a t√©cnica apropiada para proyectos gubernamentales\n",
    "3. Estructura similar a t√≠tulos de proyectos oficiales\n",
    "4. La misma brecha/problema que se atiende\n",
    "\n",
    "PROYECTO ORIGINAL:\n",
    "{proyecto_original}\n",
    "\n",
    "INSTRUCCIONES:\n",
    "- Cambia palabras por sin√≥nimos t√©cnicos (construcci√≥n‚Üíedificaci√≥n, mejoramiento‚Üíampliaci√≥n, etc.)\n",
    "- Var√≠a el orden de las frases cuando sea natural\n",
    "- Mant√©n nombres de lugares exactos\n",
    "- NO cambies el tipo de proyecto (educaci√≥n sigue siendo educaci√≥n)\n",
    "\n",
    "Genera {num_variaciones} variaciones, una por l√≠nea, sin numeraci√≥n ni explicaciones.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.8,  # Mayor temperatura = m√°s variedad\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        texto = response.choices[0].message.content.strip()\n",
    "        variaciones = [v.strip() for v in texto.split('\\n') if v.strip()]\n",
    "        \n",
    "        return variaciones[:num_variaciones]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generando variaciones: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# HERRAMIENTA 2: PARAFRASEO POR SIN√ìNIMOS (Sin API)\n",
    "# ====================================================================\n",
    "\n",
    "def generar_variaciones_sinonimos(proyecto_original, num_variaciones=2):\n",
    "    \"\"\"\n",
    "    Genera variaciones reemplazando palabras clave por sin√≥nimos.\n",
    "    M√©todo simple sin necesidad de API externa.\n",
    "    \n",
    "    Args:\n",
    "        proyecto_original (str): Texto del proyecto original\n",
    "        num_variaciones (int): N√∫mero de variaciones a generar\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de variaciones del proyecto\n",
    "    \"\"\"\n",
    "    # Diccionario de sin√≥nimos comunes en proyectos p√∫blicos\n",
    "    sinonimos = {\n",
    "        'CONSTRUCCION': ['EDIFICACION', 'INSTALACION', 'CREACION'],\n",
    "        'MEJORAMIENTO': ['AMPLIACION', 'MODERNIZACION', 'OPTIMIZACION'],\n",
    "        'REHABILITACION': ['RECUPERACION', 'RESTAURACION', 'REPARACION'],\n",
    "        'AMPLIACION': ['EXPANSION', 'EXTENSION', 'MEJORAMIENTO'],\n",
    "        'INSTALACION': ['IMPLEMENTACION', 'ESTABLECIMIENTO', 'CREACION'],\n",
    "        'AULAS': ['SALONES', 'AMBIENTES EDUCATIVOS', 'ESPACIOS EDUCATIVOS'],\n",
    "        'COLEGIO': ['INSTITUCION EDUCATIVA', 'ESCUELA', 'CENTRO EDUCATIVO'],\n",
    "        'PISTAS': ['VIAS', 'CALZADAS', 'CARPETA ASFALTICA'],\n",
    "        'VEREDAS': ['ACERAS', 'INFRAESTRUCTURA PEATONAL', 'SENDEROS PEATONALES'],\n",
    "        'AGUA POTABLE': ['SERVICIO DE AGUA', 'ABASTECIMIENTO DE AGUA', 'AGUA PARA CONSUMO'],\n",
    "        'ALCANTARILLADO': ['DESAGUE', 'SANEAMIENTO', 'SISTEMA DE DESAGUE'],\n",
    "    }\n",
    "    \n",
    "    variaciones = []\n",
    "    import random\n",
    "    \n",
    "    for i in range(num_variaciones):\n",
    "        texto_variado = proyecto_original\n",
    "        \n",
    "        # Reemplazar palabras clave por sin√≥nimos\n",
    "        palabras_cambiadas = 0\n",
    "        for palabra_original, lista_sinonimos in sinonimos.items():\n",
    "            if palabra_original in texto_variado.upper():\n",
    "                # Elegir un sin√≥nimo aleatorio\n",
    "                sinonimo = random.choice(lista_sinonimos)\n",
    "                \n",
    "                # Reemplazar (m√°ximo 2 cambios por variaci√≥n para mantener coherencia)\n",
    "                if palabras_cambiadas < 2:\n",
    "                    texto_variado = texto_variado.replace(palabra_original, sinonimo)\n",
    "                    texto_variado = texto_variado.replace(palabra_original.capitalize(), sinonimo.capitalize())\n",
    "                    palabras_cambiadas += 1\n",
    "        \n",
    "        # Solo agregar si hubo cambios\n",
    "        if texto_variado != proyecto_original:\n",
    "            variaciones.append(texto_variado)\n",
    "    \n",
    "    return variaciones\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# HERRAMIENTA 3: AUMENTAR DATASET COMPLETO\n",
    "# ====================================================================\n",
    "\n",
    "def aumentar_dataset(proyectos_df, brechas_col='brecha_ids', text_cols=['title', 'description'], \n",
    "                     num_variaciones=2, metodo='sinonimos', api_key=None):\n",
    "    \"\"\"\n",
    "    Aumenta el dataset completo generando variaciones de cada proyecto.\n",
    "    \n",
    "    Args:\n",
    "        proyectos_df (DataFrame): DataFrame original de proyectos\n",
    "        brechas_col (str): Nombre de la columna con IDs de brechas\n",
    "        text_cols (list): Columnas de texto a concatenar\n",
    "        num_variaciones (int): Variaciones por proyecto\n",
    "        metodo (str): 'gpt' o 'sinonimos'\n",
    "        api_key (str): API key de OpenAI (solo para m√©todo 'gpt')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Dataset aumentado con proyectos originales + variaciones\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    nuevos_proyectos = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"AUMENTANDO DATASET: {len(proyectos_df)} proyectos √ó {num_variaciones} variaciones\")\n",
    "    print(f\"M√©todo: {metodo}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx, row in tqdm(proyectos_df.iterrows(), total=len(proyectos_df), desc=\"Generando variaciones\"):\n",
    "        # Texto original\n",
    "        texto_original = \". \".join([str(row[col]) for col in text_cols if pd.notna(row[col])])\n",
    "        \n",
    "        # Generar variaciones\n",
    "        if metodo == 'gpt':\n",
    "            variaciones = generar_variaciones_gpt(texto_original, num_variaciones, api_key)\n",
    "        else:\n",
    "            variaciones = generar_variaciones_sinonimos(texto_original, num_variaciones)\n",
    "        \n",
    "        # Agregar cada variaci√≥n como nuevo proyecto\n",
    "        for i, variacion in enumerate(variaciones):\n",
    "            nuevo_proyecto = row.copy()\n",
    "            nuevo_proyecto['title'] = variacion.split('.')[0]  # Primera parte como t√≠tulo\n",
    "            nuevo_proyecto['description'] = variacion  # Completo como descripci√≥n\n",
    "            nuevo_proyecto['project_id'] = f\"{row['project_id']}_aug{i+1}\"  # ID √∫nico\n",
    "            nuevos_proyectos.append(nuevo_proyecto)\n",
    "    \n",
    "    # Crear DataFrame con proyectos aumentados\n",
    "    df_aumentado = pd.DataFrame(nuevos_proyectos)\n",
    "    \n",
    "    # Combinar con dataset original\n",
    "    df_completo = pd.concat([proyectos_df, df_aumentado], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset aumentado:\")\n",
    "    print(f\"  - Proyectos originales: {len(proyectos_df)}\")\n",
    "    print(f\"  - Proyectos sint√©ticos: {len(df_aumentado)}\")\n",
    "    print(f\"  - Total: {len(df_completo)}\")\n",
    "    \n",
    "    return df_completo\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# EJEMPLO DE USO\n",
    "# ====================================================================\n",
    "\n",
    "# EJEMPLO 1: Generar variaciones de un proyecto espec√≠fico con sin√≥nimos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EJEMPLO: Parafraseo por Sin√≥nimos (sin API)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "proyecto_ejemplo = \"CONSTRUCCION DE AULAS Y MEJORAMIENTO DE VEREDAS EN COLEGIO PRIMARIO\"\n",
    "print(f\"\\nProyecto original:\\n  {proyecto_ejemplo}\\n\")\n",
    "\n",
    "variaciones_sin = generar_variaciones_sinonimos(proyecto_ejemplo, num_variaciones=3)\n",
    "print(\"Variaciones generadas:\")\n",
    "for i, var in enumerate(variaciones_sin, 1):\n",
    "    print(f\"  {i}. {var}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# EJEMPLO 2: Si tienes OpenAI API key, puedes usar GPT (descomentar para probar)\n",
    "\"\"\"\n",
    "print(\"\\nEJEMPLO: Parafraseo con GPT (requiere API key)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "variaciones_gpt = generar_variaciones_gpt(proyecto_ejemplo, num_variaciones=3)\n",
    "if variaciones_gpt:\n",
    "    print(\"Variaciones generadas por GPT:\")\n",
    "    for i, var in enumerate(variaciones_gpt, 1):\n",
    "        print(f\"  {i}. {var}\")\n",
    "\"\"\"\n",
    "\n",
    "# EJEMPLO 3: Aumentar dataset completo (descomentar para ejecutar)\n",
    "\"\"\"\n",
    "# Aumentar dataset con sin√≥nimos (2 variaciones por proyecto)\n",
    "proyectos_aumentados = aumentar_dataset(\n",
    "    proyectos_test, \n",
    "    num_variaciones=2, \n",
    "    metodo='sinonimos'\n",
    ")\n",
    "\n",
    "# Guardar dataset aumentado\n",
    "proyectos_aumentados.to_csv(\"data/proyectos_aumentados.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Dataset guardado en: data/proyectos_aumentados.csv\")\n",
    "\n",
    "# IMPORTANTE: Despu√©s de generar el dataset aumentado, debes:\n",
    "# 1. Ejecutar CELDA 5 nuevamente con el dataset aumentado\n",
    "# 2. Ejecutar CELDA 6 nuevamente\n",
    "# 3. Ejecutar CELDA 7 (re-entrenar BERT con m√°s datos)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è NOTA IMPORTANTE:\")\n",
    "print(\"El data augmentation es √∫til como COMPLEMENTO, no como reemplazo de datos reales.\")\n",
    "print(\"Prioriza buscar proyectos reales en bases de datos gubernamentales.\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4e8c7",
   "metadata": {},
   "source": [
    "## üîß **Herramientas de Data Augmentation**\n",
    "\n",
    "Si decides aumentar artificialmente tu dataset, aqu√≠ hay opciones:\n",
    "\n",
    "### **Opci√≥n A: Parafraseo con GPT (Recomendado)**\n",
    "Genera variaciones autom√°ticas de proyectos existentes manteniendo la brecha.\n",
    "\n",
    "### **Opci√≥n B: Traducci√≥n Inversa (Back-Translation)**\n",
    "Traducir espa√±ol ‚Üí ingl√©s ‚Üí espa√±ol para obtener variaciones naturales.\n",
    "\n",
    "### **Opci√≥n C: Reemplazo de Sin√≥nimos**\n",
    "Cambiar palabras clave por sin√≥nimos:\n",
    "- \"construcci√≥n\" ‚Üî \"edificaci√≥n\"\n",
    "- \"mejoramiento\" ‚Üî \"ampliaci√≥n\"\n",
    "- \"aulas\" ‚Üî \"salones educativos\"\n",
    "\n",
    "### **Opci√≥n D: B√∫squeda de Proyectos Reales**\n",
    "Fuentes gubernamentales de Per√∫:\n",
    "- **SNIP**: Sistema Nacional de Inversi√≥n P√∫blica\n",
    "- **Invierte.pe**: Banco de proyectos\n",
    "- **MEF Transparencia**: Portal de transparencia econ√≥mica\n",
    "- **SEACE**: Sistema Electr√≥nico de Contrataciones\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è Reglas de Oro para Data Augmentation:**\n",
    "\n",
    "1. **Mantener coherencia sem√°ntica**: Las variaciones deben tener sentido\n",
    "2. **No cambiar la brecha**: Un proyecto de educaci√≥n no debe convertirse en salud\n",
    "3. **Validar manualmente**: Revisar muestras de datos generados\n",
    "4. **Limitar ratio**: M√°ximo 3-5 variaciones por proyecto original\n",
    "5. **Priorizar datos reales**: Augmentation es complemento, no reemplazo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a869575d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (194536428.py, line 33)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"Usa LLM si disponible; si no, fallback por mayor final_score.\"\"\"\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# CELDA 9 - Integraci√≥n con LLM (Opcional)\n",
    "# Requiere variable de entorno OPENAI_API_KEY (o adapta a otro proveedor)\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    _openai_available = True\n",
    "except ImportError:\n",
    "    _openai_available = False\n",
    "\n",
    "\n",
    "def build_llm_prompt(project_text: str, results: List[Dict]) -> str:\n",
    "    prompt = [\n",
    "        \"Eres un asistente experto en clasificaci√≥n de proyectos p√∫blicos seg√∫n brechas.\",\n",
    "        \"Analiza el proyecto y los candidatos (brechas) con sus puntajes combinados.\",\n",
    "        \"Devuelve JSON con las claves: brecha_id, brecha_text, razon, confiabilidad (0-1).\",\n",
    "        \"Si ning√∫n candidato es adecuado, usa brecha_id=null y explica brevemente.\",\n",
    "        \"Mant√©n la explicaci√≥n concisa (m√°ximo 3 frases).\",\n",
    "        \"\\nProyecto:\", project_text, \"\\nCandidatos:\"  # newline groups\n",
    "    ]\n",
    "    for r in results:\n",
    "        prompt.append(f\"- ID {r['brecha_id']}: {r['brecha_text']} | final={r['final_score']:.3f} bert={r['bert_score']:.3f} embed={r['embed_score']:.3f}\")\n",
    "    prompt.append(\"\\nElige la mejor brecha y justifica.\")\n",
    "    return \"\\n\".join(prompt)\n",
    "\n",
    "\n",
    "def classify_with_llm(project_text: str, results: List[Dict], model: str = \"gpt-4o-mini\") -> Dict:\n",
    "    \"\"\"Usa LLM si disponible; si no, fallback por mayor final_score.\"\"\"\n",
    "    if not results:\n",
    "        return {\"brecha_id\": None, \"brecha_text\": None, \"razon\": \"No hay candidatos disponibles.\", \"confiabilidad\": 0.0}\n",
    "\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if api_key and _openai_available:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        prompt = build_llm_prompt(project_text, results)\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"system\", \"content\": \"Eres un asistente de clasificaci√≥n\"},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.2,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            text = completion.choices[0].message.content.strip()\n",
    "            # Intentar parsear JSON si el modelo lo produjo\n",
    "            if text.startswith(\"{\"):\n",
    "                try:\n",
    "                    parsed = json.loads(text)\n",
    "                    return {\n",
    "                        \"brecha_id\": parsed.get(\"brecha_id\"),\n",
    "                        \"brecha_text\": parsed.get(\"brecha_text\"),\n",
    "                        \"razon\": parsed.get(\"razon\"),\n",
    "                        \"confiabilidad\": parsed.get(\"confiabilidad\", 0.0),\n",
    "                        \"raw\": text\n",
    "                    }\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "            # Si no es JSON, fallback a heur√≠stica + incluir respuesta cruda\n",
    "            best = max(results, key=lambda x: x['final_score'])\n",
    "            return {\n",
    "                \"brecha_id\": best['brecha_id'],\n",
    "                \"brecha_text\": best['brecha_text'],\n",
    "                \"razon\": f\"Selecci√≥n heur√≠stica (respuesta LLM no estructurada): {text[:160]}\",\n",
    "                \"confiabilidad\": min(1.0, best['final_score']),\n",
    "                \"raw\": text\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"[LLM ERROR] {e}; usando fallback heur√≠stico.\")\n",
    "\n",
    "    # Fallback sin API\n",
    "    best = max(results, key=lambda x: x['final_score'])\n",
    "    return {\n",
    "        \"brecha_id\": best['brecha_id'],\n",
    "        \"brecha_text\": best['brecha_text'],\n",
    "        \"razon\": \"Seleccionada por mayor puntaje h√≠brido (sin LLM).\",\n",
    "        \"confiabilidad\": min(1.0, best['final_score'])\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso (despu√©s de ejecutar CELDA 8):\n",
    "# project_text = \"AMPLIACION DE 01 AULA + 01 DIRECCION EN LA CUNA JARDIN, HOSPITAL AMAZONICO- PUERTO CALLAO\"\n",
    "# candidates = hybrid_classify(project_text, top_k=5)\n",
    "# resultado_llm = classify_with_llm(project_text, candidates)\n",
    "# print(resultado_llm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
